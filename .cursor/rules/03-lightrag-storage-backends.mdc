---
description: LightRAG å­˜å‚¨åç«¯å¼€å‘è§„åˆ™
alwaysApply: false
---
# LightRAG å­˜å‚¨åç«¯å¼€å‘è§„åˆ™ (2025ç‰ˆ)

## ğŸ—„ï¸ å­˜å‚¨æ¶æ„æ¦‚è§ˆ

LightRAGé‡‡ç”¨åˆ†å±‚å­˜å‚¨æ¶æ„ï¼Œæ”¯æŒå¤šç§å­˜å‚¨åç«¯çš„æ— ç¼åˆ‡æ¢ã€‚æ ¹æ®ç”¨æˆ·é…ç½®ï¼Œå½“å‰ä½¿ç”¨ï¼š
- **KVå­˜å‚¨**: RedisKVStorage (é”®å€¼å¯¹å­˜å‚¨)
- **æ–‡æ¡£çŠ¶æ€**: PGDocStatusStorage (PostgreSQL)
- **å›¾å­˜å‚¨**: Neo4JStorage (Neo4jå›¾æ•°æ®åº“) 
- **å‘é‡å­˜å‚¨**: PGVectorStorage (PostgreSQL + pgvector)

æ”¯æŒä»æœ¬åœ°å¼€å‘åˆ°ç”Ÿäº§ç¯å¢ƒçš„å¹³æ»‘è¿ç§»ã€‚

## ğŸ“‚ å­˜å‚¨æ¨¡å—ç»“æ„

```
lightrag/kg/
â”œâ”€â”€ shared_storage.py           # å­˜å‚¨æ¥å£å®šä¹‰å’Œç®¡ç†å™¨
â”œâ”€â”€ json_kv_impl.py            # JSONæœ¬åœ°å­˜å‚¨ (å¼€å‘)
â”œâ”€â”€ json_doc_status_impl.py    # JSONæ–‡æ¡£çŠ¶æ€å­˜å‚¨
â”œâ”€â”€ redis_impl.py              # Rediså®ç° (ç”Ÿäº§æ¨è)
â”œâ”€â”€ postgres_impl.py           # PostgreSQLå®ç° (å‘é‡+æ–‡æ¡£)
â”œâ”€â”€ neo4j_impl.py              # Neo4jå®ç° (å›¾å­˜å‚¨)
â”œâ”€â”€ mongo_impl.py              # MongoDBå®ç°
â”œâ”€â”€ milvus_impl.py             # Milvuså‘é‡æ•°æ®åº“
â”œâ”€â”€ qdrant_impl.py             # Qdrantå‘é‡æ•°æ®åº“
â”œâ”€â”€ faiss_impl.py              # Faisså‘é‡ç´¢å¼• (æœ¬åœ°)
â”œâ”€â”€ memgraph_impl.py           # Memgraphå›¾æ•°æ®åº“
â””â”€â”€ nano_vector_db_impl.py     # è½»é‡çº§å‘é‡æ•°æ®åº“
```

## ğŸ”§ 2025å¹´å­˜å‚¨æ¥å£è§„èŒƒ

### å¼‚æ­¥å­˜å‚¨åŸºç±»è®¾è®¡
```python
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional, Union, AsyncGenerator
import asyncio
import logging
from contextlib import asynccontextmanager

logger = logging.getLogger(__name__)

class BaseStorageManager(ABC):
    """2025å¹´å¼‚æ­¥å­˜å‚¨ç®¡ç†å™¨åŸºç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.is_initialized = False
        self.connection_pool: Optional[Any] = None
        
    async def __aenter__(self):
        await self.initialize()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.close()
    
    @abstractmethod
    async def initialize(self) -> None:
        """åˆå§‹åŒ–å­˜å‚¨è¿æ¥"""
        pass
    
    @abstractmethod
    async def close(self) -> None:
        """å…³é—­å­˜å‚¨è¿æ¥"""
        pass
    
    @abstractmethod
    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        pass
    
    # å¼‚æ­¥æ‰¹é‡æ“ä½œæ¥å£
    @abstractmethod
    async def batch_get(self, keys: List[str]) -> Dict[str, Any]:
        """æ‰¹é‡è·å–æ•°æ®"""
        pass
    
    @abstractmethod
    async def batch_set(self, data: Dict[str, Any]) -> bool:
        """æ‰¹é‡è®¾ç½®æ•°æ®"""
        pass
    
    @abstractmethod
    async def batch_delete(self, keys: List[str]) -> bool:
        """æ‰¹é‡åˆ é™¤æ•°æ®"""
        pass
    
    # æµå¼æ“ä½œæ¥å£ (2025æ–°å¢)
    @abstractmethod
    async def stream_keys(self, pattern: str = "*") -> AsyncGenerator[str, None]:
        """æµå¼è·å–é”®å"""
        pass
    
    @abstractmethod
    async def stream_values(self, keys: List[str]) -> AsyncGenerator[Any, None]:
        """æµå¼è·å–å€¼"""
        pass
```

## ğŸ”´ Rediså¼‚æ­¥å®ç° (2025ä¼˜åŒ–ç‰ˆ)

### Redisè¿æ¥æ± ç®¡ç†
```python
import redis.asyncio as redis
import json
import asyncio
from typing import Optional, Dict, Any, List
import time

class RedisKVStorage(BaseStorageManager):
    """2025å¹´Rediså¼‚æ­¥å­˜å‚¨å®ç°"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.pool: Optional[redis.ConnectionPool] = None
        self.client: Optional[redis.Redis] = None
        self.cluster_mode = config.get("cluster_mode", False)
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "operations": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "errors": 0,
            "avg_response_time": 0.0
        }
    
    async def initialize(self) -> None:
        """åˆå§‹åŒ–Redisè¿æ¥æ±  - 2025æœ€ä½³é…ç½®"""
        try:
            if self.cluster_mode:
                # Redis Clusteræ¨¡å¼
                from redis.asyncio.cluster import RedisCluster
                self.client = RedisCluster(
                    startup_nodes=[
                        {"host": node["host"], "port": node["port"]}
                        for node in self.config.get("cluster_nodes", [])
                    ],
                    decode_responses=True,
                    skip_full_coverage_check=True,
                    max_connections_per_node=20,
                    retry_on_timeout=True,
                    health_check_interval=30
                )
            else:
                # å•æœºæ¨¡å¼è¿æ¥æ± 
                self.pool = redis.ConnectionPool.from_url(
                    self.config.get("url", "redis://localhost:6379/0"),
                    max_connections=50,           # è¿æ¥æ± å¤§å°
                    socket_connect_timeout=5,     # è¿æ¥è¶…æ—¶
                    socket_timeout=30,            # æ“ä½œè¶…æ—¶
                    retry_on_timeout=True,        # è¶…æ—¶é‡è¯•
                    health_check_interval=30,     # å¥åº·æ£€æŸ¥é—´éš”
                    encoding='utf-8',
                    decode_responses=True
                )
                self.client = redis.Redis.from_pool(self.pool)
            
            # æµ‹è¯•è¿æ¥
            await self.client.ping()
            self.is_initialized = True
            logger.info("Redisè¿æ¥åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"Redisè¿æ¥åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def close(self) -> None:
        """æ­£ç¡®å…³é—­Redisè¿æ¥"""
        if self.client:
            await self.client.aclose()
        if self.pool:
            await self.pool.aclose()
        self.is_initialized = False
        logger.info("Redisè¿æ¥å·²å…³é—­")
    
    async def health_check(self) -> bool:
        """Rediså¥åº·æ£€æŸ¥"""
        try:
            start_time = time.time()
            result = await asyncio.wait_for(self.client.ping(), timeout=5.0)
            response_time = time.time() - start_time
            
            # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
            self.stats["avg_response_time"] = (
                self.stats["avg_response_time"] * 0.9 + response_time * 0.1
            )
            
            return result
        except Exception as e:
            logger.error(f"Rediså¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            self.stats["errors"] += 1
            return False
    
    # é«˜æ€§èƒ½å¼‚æ­¥æ“ä½œå®ç°
    async def get(self, key: str) -> Optional[Any]:
        """å¼‚æ­¥è·å–å•ä¸ªé”®å€¼"""
        start_time = time.time()
        try:
            self.stats["operations"] += 1
            value = await self.client.get(key)
            
            if value is not None:
                self.stats["cache_hits"] += 1
                # å°è¯•è§£æJSONï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›åŸå§‹å­—ç¬¦ä¸²
                try:
                    return json.loads(value)
                except (json.JSONDecodeError, TypeError):
                    return value
            else:
                self.stats["cache_misses"] += 1
                return None
                
        except Exception as e:
            self.stats["errors"] += 1
            logger.error(f"Redis GETé”™è¯¯ {key}: {e}")
            return None
        finally:
            # æ›´æ–°å“åº”æ—¶é—´ç»Ÿè®¡
            response_time = time.time() - start_time
            self.stats["avg_response_time"] = (
                self.stats["avg_response_time"] * 0.9 + response_time * 0.1
            )
    
    async def set(self, key: str, value: Any, ex: Optional[int] = None) -> bool:
        """å¼‚æ­¥è®¾ç½®é”®å€¼å¯¹"""
        try:
            self.stats["operations"] += 1
            
            # åºåˆ—åŒ–å€¼
            if isinstance(value, (dict, list)):
                serialized_value = json.dumps(value, ensure_ascii=False)
            else:
                serialized_value = str(value)
            
            # è®¾ç½®è¿‡æœŸæ—¶é—´
            if ex is not None:
                result = await self.client.set(key, serialized_value, ex=ex)
            else:
                result = await self.client.set(key, serialized_value)
            
            return bool(result)
            
        except Exception as e:
            self.stats["errors"] += 1
            logger.error(f"Redis SETé”™è¯¯ {key}: {e}")
            return False
    
    async def batch_get(self, keys: List[str]) -> Dict[str, Any]:
        """æ‰¹é‡è·å– - 2025ä¼˜åŒ–ç‰ˆ"""
        if not keys:
            return {}
        
        try:
            self.stats["operations"] += len(keys)
            
            # ä½¿ç”¨mgetæ‰¹é‡è·å–
            values = await self.client.mget(keys)
            
            result = {}
            for key, value in zip(keys, values):
                if value is not None:
                    self.stats["cache_hits"] += 1
                    try:
                        result[key] = json.loads(value)
                    except (json.JSONDecodeError, TypeError):
                        result[key] = value
                else:
                    self.stats["cache_misses"] += 1
            
            return result
            
        except Exception as e:
            self.stats["errors"] += len(keys)
            logger.error(f"Redisæ‰¹é‡GETé”™è¯¯: {e}")
            return {}
    
    async def batch_set(self, data: Dict[str, Any], ex: Optional[int] = None) -> bool:
        """æ‰¹é‡è®¾ç½® - ä½¿ç”¨pipelineä¼˜åŒ–"""
        if not data:
            return True
        
        try:
            self.stats["operations"] += len(data)
            
            # ä½¿ç”¨pipelineæ‰¹é‡è®¾ç½®
            async with self.client.pipeline(transaction=True) as pipe:
                for key, value in data.items():
                    # åºåˆ—åŒ–å€¼
                    if isinstance(value, (dict, list)):
                        serialized_value = json.dumps(value, ensure_ascii=False)
                    else:
                        serialized_value = str(value)
                    
                    if ex is not None:
                        pipe.set(key, serialized_value, ex=ex)
                    else:
                        pipe.set(key, serialized_value)
                
                results = await pipe.execute()
                return all(results)
                
        except Exception as e:
            self.stats["errors"] += len(data)
            logger.error(f"Redisæ‰¹é‡SETé”™è¯¯: {e}")
            return False
    
    async def batch_delete(self, keys: List[str]) -> bool:
        """æ‰¹é‡åˆ é™¤"""
        if not keys:
            return True
        
        try:
            self.stats["operations"] += len(keys)
            deleted_count = await self.client.delete(*keys)
            return deleted_count > 0
            
        except Exception as e:
            self.stats["errors"] += len(keys)
            logger.error(f"Redisæ‰¹é‡DELETEé”™è¯¯: {e}")
            return False
    
    async def stream_keys(self, pattern: str = "*") -> AsyncGenerator[str, None]:
        """æµå¼è·å–é”®å - ä½¿ç”¨SCANé¿å…é˜»å¡"""
        try:
            cursor = 0
            while True:
                cursor, keys = await self.client.scan(
                    cursor=cursor, 
                    match=pattern, 
                    count=100  # æ¯æ¬¡æ‰«æ100ä¸ªé”®
                )
                
                for key in keys:
                    yield key
                
                if cursor == 0:  # æ‰«æå®Œæ¯•
                    break
                    
                # é¿å…è¿‡å¿«è¿­ä»£ï¼Œç»™å…¶ä»–æ“ä½œè®©å‡ºæ—¶é—´
                await asyncio.sleep(0.001)
                
        except Exception as e:
            logger.error(f"Redis SCANé”™è¯¯: {e}")
    
    async def stream_values(self, keys: List[str]) -> AsyncGenerator[Any, None]:
        """æµå¼è·å–å€¼ - åˆ†æ‰¹å¤„ç†å¤§é‡é”®"""
        batch_size = 100
        
        for i in range(0, len(keys), batch_size):
            batch_keys = keys[i:i + batch_size]
            batch_values = await self.batch_get(batch_keys)
            
            for key in batch_keys:
                yield batch_values.get(key)
            
            # é¿å…é˜»å¡äº‹ä»¶å¾ªç¯
            await asyncio.sleep(0.001)
    
    # 2025æ–°å¢ï¼šæ€§èƒ½ç›‘æ§æ–¹æ³•
    async def get_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        total_ops = self.stats["operations"]
        hit_rate = (
            self.stats["cache_hits"] / total_ops * 100 
            if total_ops > 0 else 0
        )
        
        return {
            **self.stats,
            "cache_hit_rate": f"{hit_rate:.2f}%",
            "error_rate": f"{self.stats['errors'] / total_ops * 100:.2f}%" if total_ops > 0 else "0%",
            "avg_response_time_ms": f"{self.stats['avg_response_time'] * 1000:.2f}ms"
        }
    
    async def reset_stats(self):
        """é‡ç½®æ€§èƒ½ç»Ÿè®¡"""
        self.stats = {
            "operations": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "errors": 0,
            "avg_response_time": 0.0
        }
```

## ğŸ˜ PostgreSQLå¼‚æ­¥å®ç° (2025ä¼˜åŒ–ç‰ˆ)

### PostgreSQL + pgvectoræœ€ä½³å®è·µ
```python
import asyncpg
import numpy as np
import json
from typing import List, Dict, Any, Optional, Tuple
import time

class PGVectorStorage(BaseStorageManager):
    """2025å¹´PostgreSQL + pgvectorå¼‚æ­¥å®ç°"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.pool: Optional[asyncpg.Pool] = None
        self.vector_dim = config.get("vector_dim", 1024)
        
    async def initialize(self) -> None:
        """åˆå§‹åŒ–PostgreSQLè¿æ¥æ± """
        try:
            # 2025å¹´ä¼˜åŒ–çš„è¿æ¥æ± é…ç½®
            self.pool = await asyncpg.create_pool(
                host=self.config.get("host", "localhost"),
                port=self.config.get("port", 5432),
                user=self.config.get("user", "postgres"),
                password=self.config.get("password"),
                database=self.config.get("database", "lightrag_db"),
                min_size=5,                    # æœ€å°è¿æ¥æ•°
                max_size=50,                   # æœ€å¤§è¿æ¥æ•°
                command_timeout=60,            # å‘½ä»¤è¶…æ—¶
                server_settings={
                    'jit': 'off',              # ç¦ç”¨JITæå‡å°æŸ¥è¯¢æ€§èƒ½
                    'shared_preload_libraries': 'vector,pg_stat_statements',
                    'max_connections': '200',
                    'work_mem': '256MB',       # æå‡æ’åºæ€§èƒ½
                    'maintenance_work_mem': '1GB',  # æå‡ç´¢å¼•æ„å»ºæ€§èƒ½
                    'effective_cache_size': '4GB',  # ç¼“å­˜å¤§å°
                    'random_page_cost': '1.1',     # SSDä¼˜åŒ–
                    'effective_io_concurrency': '200'  # å¹¶å‘I/O
                }
            )
            
            await self._create_extensions_and_tables()
            self.is_initialized = True
            logger.info("PostgreSQL + pgvectorè¿æ¥åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"PostgreSQLè¿æ¥åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def _create_extensions_and_tables(self):
        """åˆ›å»ºæ‰©å±•å’Œè¡¨ç»“æ„"""
        async with self.pool.acquire() as conn:
            # åˆ›å»ºæ‰©å±•
            await conn.execute("CREATE EXTENSION IF NOT EXISTS vector;")
            await conn.execute("CREATE EXTENSION IF NOT EXISTS pg_stat_statements;")
            
            # åˆ›å»ºå‘é‡è¡¨
            await conn.execute(f"""
                CREATE TABLE IF NOT EXISTS embeddings (
                    id SERIAL PRIMARY KEY,
                    content TEXT NOT NULL,
                    embedding vector({self.vector_dim}),
                    metadata JSONB DEFAULT '{{}}',
                    created_at TIMESTAMP DEFAULT NOW(),
                    updated_at TIMESTAMP DEFAULT NOW()
                );
            """)
            
            # åˆ›å»ºç´¢å¼• - 2025å¹´ä¼˜åŒ–é…ç½®
            await conn.execute("""
                CREATE INDEX IF NOT EXISTS embeddings_vector_cosine_idx 
                ON embeddings USING ivfflat (embedding vector_cosine_ops) 
                WITH (lists = 1000);
            """)
            
            await conn.execute("""
                CREATE INDEX IF NOT EXISTS embeddings_metadata_idx 
                ON embeddings USING GIN (metadata);
            """)
            
            await conn.execute("""
                CREATE INDEX IF NOT EXISTS embeddings_created_at_idx 
                ON embeddings (created_at DESC);
            """)
            
            # åˆ›å»ºæ›´æ–°æ—¶é—´è§¦å‘å™¨
            await conn.execute("""
                CREATE OR REPLACE FUNCTION update_updated_at_column()
                RETURNS TRIGGER AS $$
                BEGIN
                    NEW.updated_at = NOW();
                    RETURN NEW;
                END;
                $$ language 'plpgsql';
            """)
            
            await conn.execute("""
                DROP TRIGGER IF EXISTS update_embeddings_updated_at ON embeddings;
                CREATE TRIGGER update_embeddings_updated_at 
                BEFORE UPDATE ON embeddings 
                FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
            """)
    
    async def close(self) -> None:
        """å…³é—­è¿æ¥æ± """
        if self.pool:
            await self.pool.close()
        self.is_initialized = False
        logger.info("PostgreSQLè¿æ¥å·²å…³é—­")
    
    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        try:
            async with self.pool.acquire() as conn:
                result = await conn.fetchval("SELECT 1")
                return result == 1
        except Exception as e:
            logger.error(f"PostgreSQLå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    async def upsert_embeddings(
        self, 
        data: List[Tuple[str, np.ndarray, Dict[str, Any]]]
    ) -> bool:
        """æ‰¹é‡æ’å…¥/æ›´æ–°å‘é‡æ•°æ® - 2025ä¼˜åŒ–ç‰ˆ"""
        if not data:
            return True
        
        try:
            async with self.pool.acquire() as conn:
                # ä½¿ç”¨COPYè¿›è¡Œé«˜æ€§èƒ½æ‰¹é‡æ’å…¥
                records = []
                for content, embedding, metadata in data:
                    records.append((
                        content,
                        embedding.tolist(),  # è½¬æ¢ä¸ºåˆ—è¡¨
                        json.dumps(metadata)
                    ))
                
                # ä½¿ç”¨ON CONFLICTè¿›è¡Œupsert
                await conn.executemany("""
                    INSERT INTO embeddings (content, embedding, metadata)
                    VALUES ($1, $2::vector, $3::jsonb)
                    ON CONFLICT (content) DO UPDATE SET
                        embedding = EXCLUDED.embedding,
                        metadata = EXCLUDED.metadata,
                        updated_at = NOW()
                """, records)
                
                return True
                
        except Exception as e:
            logger.error(f"PostgreSQLæ‰¹é‡upserté”™è¯¯: {e}")
            return False
    
    async def similarity_search(
        self, 
        query_vector: np.ndarray, 
        top_k: int = 10,
        threshold: float = 0.7,
        metadata_filter: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """å‘é‡ç›¸ä¼¼åº¦æœç´¢ - 2025ä¼˜åŒ–ç‰ˆ"""
        try:
            async with self.pool.acquire() as conn:
                # æ„å»ºæŸ¥è¯¢æ¡ä»¶
                where_clause = ""
                params = [query_vector.tolist(), threshold, top_k]
                
                if metadata_filter:
                    conditions = []
                    for key, value in metadata_filter.items():
                        conditions.append(f"metadata ->> '{key}' = ${len(params) + 1}")
                        params.append(str(value))
                    
                    if conditions:
                        where_clause = f"AND {' AND '.join(conditions)}"
                
                # æ‰§è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢
                query = f"""
                    SELECT 
                        id, 
                        content, 
                        metadata, 
                        1 - (embedding <=> $1::vector) as similarity,
                        created_at
                    FROM embeddings
                    WHERE 1 - (embedding <=> $1::vector) > $2
                    {where_clause}
                    ORDER BY embedding <=> $1::vector
                    LIMIT $3
                """
                
                rows = await conn.fetch(query, *params)
                
                return [
                    {
                        "id": row["id"],
                        "content": row["content"],
                        "metadata": row["metadata"],
                        "similarity": float(row["similarity"]),
                        "created_at": row["created_at"].isoformat()
                    }
                    for row in rows
                ]
                
        except Exception as e:
            logger.error(f"PostgreSQLå‘é‡æœç´¢é”™è¯¯: {e}")
            return []
    
    async def get_collection_stats(self) -> Dict[str, Any]:
        """è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯"""
        try:
            async with self.pool.acquire() as conn:
                stats = await conn.fetchrow("""
                    SELECT 
                        COUNT(*) as total_vectors,
                        AVG(ARRAY_LENGTH(embedding::float[], 1)) as avg_dimension,
                        MIN(created_at) as oldest_record,
                        MAX(created_at) as newest_record,
                        pg_size_pretty(pg_total_relation_size('embeddings')) as table_size
                    FROM embeddings
                """)
                
                return dict(stats) if stats else {}
                
        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯é”™è¯¯: {e}")
            return {}
    
    # å®ç°åŸºç±»æ–¹æ³•
    async def batch_get(self, keys: List[str]) -> Dict[str, Any]:
        """æ‰¹é‡è·å–æ–‡æ¡£"""
        try:
            async with self.pool.acquire() as conn:
                rows = await conn.fetch("""
                    SELECT content, metadata, embedding 
                    FROM embeddings 
                    WHERE content = ANY($1::text[])
                """, keys)
                
                return {
                    row["content"]: {
                        "metadata": row["metadata"],
                        "embedding": row["embedding"]
                    }
                    for row in rows
                }
                
        except Exception as e:
            logger.error(f"PostgreSQLæ‰¹é‡è·å–é”™è¯¯: {e}")
            return {}
    
    async def batch_set(self, data: Dict[str, Any]) -> bool:
        """æ‰¹é‡è®¾ç½®æ–‡æ¡£"""
        embeddings_data = [
            (key, np.array(value["embedding"]), value.get("metadata", {}))
            for key, value in data.items()
            if "embedding" in value
        ]
        return await self.upsert_embeddings(embeddings_data)
    
    async def batch_delete(self, keys: List[str]) -> bool:
        """æ‰¹é‡åˆ é™¤æ–‡æ¡£"""
        try:
            async with self.pool.acquire() as conn:
                deleted_count = await conn.execute("""
                    DELETE FROM embeddings 
                    WHERE content = ANY($1::text[])
                """, keys)
                return int(deleted_count.split()[-1]) > 0
                
        except Exception as e:
            logger.error(f"PostgreSQLæ‰¹é‡åˆ é™¤é”™è¯¯: {e}")
            return False
    
    async def stream_keys(self, pattern: str = "*") -> AsyncGenerator[str, None]:
        """æµå¼è·å–å†…å®¹é”®"""
        try:
            async with self.pool.acquire() as conn:
                async with conn.transaction():
                    # ä½¿ç”¨æœåŠ¡å™¨ç«¯æ¸¸æ ‡é¿å…å†…å­˜é—®é¢˜
                    await conn.execute("""
                        DECLARE content_cursor CURSOR FOR 
                        SELECT content FROM embeddings ORDER BY id
                    """)
                    
                    while True:
                        rows = await conn.fetch("FETCH 100 FROM content_cursor")
                        if not rows:
                            break
                        
                        for row in rows:
                            yield row["content"]
                        
                        await asyncio.sleep(0.001)
                        
        except Exception as e:
            logger.error(f"PostgreSQLæµå¼é”®è·å–é”™è¯¯: {e}")
    
    async def stream_values(self, keys: List[str]) -> AsyncGenerator[Any, None]:
        """æµå¼è·å–å€¼"""
        batch_size = 50
        for i in range(0, len(keys), batch_size):
            batch_keys = keys[i:i + batch_size]
            batch_data = await self.batch_get(batch_keys)
            
            for key in batch_keys:
                yield batch_data.get(key)
            
            await asyncio.sleep(0.001)
```

## ğŸ”— Neo4jå›¾æ•°æ®åº“å®ç° (2025ç‰ˆ)

### å¼‚æ­¥Neo4jæ“ä½œ
```python
from neo4j import AsyncGraphDatabase
from typing import List, Dict, Any, Optional
import asyncio

class Neo4JStorage(BaseStorageManager):
    """2025å¹´Neo4jå¼‚æ­¥å›¾å­˜å‚¨å®ç°"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.driver = None
        self.max_connection_lifetime = config.get("max_connection_lifetime", 3600)
        
    async def initialize(self) -> None:
        """åˆå§‹åŒ–Neo4jè¿æ¥"""
        try:
            self.driver = AsyncGraphDatabase.driver(
                self.config.get("uri", "bolt://localhost:7687"),
                auth=(
                    self.config.get("user", "neo4j"),
                    self.config.get("password", "password")
                ),
                max_connection_lifetime=self.max_connection_lifetime,
                max_connection_pool_size=50,
                connection_acquisition_timeout=60,
                encrypted=self.config.get("encrypted", False)
            )
            
            # æµ‹è¯•è¿æ¥
            await self.driver.verify_connectivity()
            
            # åˆ›å»ºç´¢å¼•
            await self._create_indexes()
            
            self.is_initialized = True
            logger.info("Neo4jè¿æ¥åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"Neo4jè¿æ¥åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def _create_indexes(self):
        """åˆ›å»ºå›¾æ•°æ®åº“ç´¢å¼•"""
        async with self.driver.session() as session:
            # å®ä½“ç´¢å¼•
            await session.run("""
                CREATE INDEX entity_name_idx IF NOT EXISTS 
                FOR (e:Entity) ON (e.name)
            """)
            
            # å…³ç³»ç´¢å¼•
            await session.run("""
                CREATE INDEX relation_type_idx IF NOT EXISTS 
                FOR ()-[r:RELATION]-() ON (r.type)
            """)
    
    async def close(self) -> None:
        """å…³é—­Neo4jè¿æ¥"""
        if self.driver:
            await self.driver.close()
        self.is_initialized = False
        logger.info("Neo4jè¿æ¥å·²å…³é—­")
    
    async def health_check(self) -> bool:
        """Neo4jå¥åº·æ£€æŸ¥"""
        try:
            await self.driver.verify_connectivity()
            return True
        except Exception as e:
            logger.error(f"Neo4jå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    async def upsert_entities(self, entities: List[Dict[str, Any]]) -> bool:
        """æ‰¹é‡æ’å…¥/æ›´æ–°å®ä½“"""
        try:
            async with self.driver.session() as session:
                await session.execute_write(self._upsert_entities_tx, entities)
                return True
        except Exception as e:
            logger.error(f"Neo4jå®ä½“upserté”™è¯¯: {e}")
            return False
    
    @staticmethod
    async def _upsert_entities_tx(tx, entities):
        """å®ä½“upsertäº‹åŠ¡"""
        for entity in entities:
            await tx.run("""
                MERGE (e:Entity {name: $name})
                SET e.description = $description,
                    e.type = $type,
                    e.properties = $properties,
                    e.updated_at = datetime()
            """, 
            name=entity["name"],
            description=entity.get("description", ""),
            type=entity.get("type", ""),
            properties=entity.get("properties", {})
            )
    
    async def upsert_relationships(self, relationships: List[Dict[str, Any]]) -> bool:
        """æ‰¹é‡æ’å…¥/æ›´æ–°å…³ç³»"""
        try:
            async with self.driver.session() as session:
                await session.execute_write(self._upsert_relationships_tx, relationships)
                return True
        except Exception as e:
            logger.error(f"Neo4jå…³ç³»upserté”™è¯¯: {e}")
            return False
    
    @staticmethod
    async def _upsert_relationships_tx(tx, relationships):
        """å…³ç³»upsertäº‹åŠ¡"""
        for rel in relationships:
            await tx.run("""
                MATCH (src:Entity {name: $src_name})
                MATCH (tgt:Entity {name: $tgt_name})
                MERGE (src)-[r:RELATION {type: $rel_type}]->(tgt)
                SET r.description = $description,
                    r.properties = $properties,
                    r.updated_at = datetime()
            """,
            src_name=rel["source"],
            tgt_name=rel["target"],
            rel_type=rel["type"],
            description=rel.get("description", ""),
            properties=rel.get("properties", {})
            )
    
    async def get_subgraph(
        self, 
        entity_names: List[str], 
        max_depth: int = 2
    ) -> Dict[str, Any]:
        """è·å–å­å›¾"""
        try:
            async with self.driver.session() as session:
                result = await session.execute_read(
                    self._get_subgraph_tx, 
                    entity_names, 
                    max_depth
                )
                return result
        except Exception as e:
            logger.error(f"Neo4jå­å›¾æŸ¥è¯¢é”™è¯¯: {e}")
            return {"nodes": [], "edges": []}
    
    @staticmethod
    async def _get_subgraph_tx(tx, entity_names, max_depth):
        """å­å›¾æŸ¥è¯¢äº‹åŠ¡"""
        # æŸ¥è¯¢èŠ‚ç‚¹å’Œå…³ç³»
        result = await tx.run(f"""
            MATCH path = (start:Entity)-[*1..{max_depth}]-(end:Entity)
            WHERE start.name IN $entity_names
            RETURN nodes(path) as nodes, relationships(path) as rels
        """, entity_names=entity_names)
        
        nodes = []
        edges = []
        node_ids = set()
        edge_ids = set()
        
        async for record in result:
            # å¤„ç†èŠ‚ç‚¹
            for node in record["nodes"]:
                if node.id not in node_ids:
                    nodes.append({
                        "id": str(node.id),
                        "name": node["name"],
                        "type": node.get("type", ""),
                        "description": node.get("description", ""),
                        "properties": dict(node.get("properties", {}))
                    })
                    node_ids.add(node.id)
            
            # å¤„ç†å…³ç³»
            for rel in record["rels"]:
                if rel.id not in edge_ids:
                    edges.append({
                        "id": str(rel.id),
                        "source": str(rel.start_node.id),
                        "target": str(rel.end_node.id),
                        "type": rel["type"],
                        "description": rel.get("description", ""),
                        "properties": dict(rel.get("properties", {}))
                    })
                    edge_ids.add(rel.id)
        
        return {"nodes": nodes, "edges": edges}
    
    # å®ç°åŸºç±»æ–¹æ³•
    async def batch_get(self, keys: List[str]) -> Dict[str, Any]:
        """æ‰¹é‡è·å–å®ä½“"""
        try:
            async with self.driver.session() as session:
                result = await session.execute_read(self._batch_get_entities_tx, keys)
                return result
        except Exception as e:
            logger.error(f"Neo4jæ‰¹é‡è·å–é”™è¯¯: {e}")
            return {}
    
    @staticmethod
    async def _batch_get_entities_tx(tx, entity_names):
        """æ‰¹é‡è·å–å®ä½“äº‹åŠ¡"""
        result = await tx.run("""
            MATCH (e:Entity)
            WHERE e.name IN $names
            RETURN e.name as name, e
        """, names=entity_names)
        
        entities = {}
        async for record in result:
            entity = record["e"]
            entities[record["name"]] = {
                "id": str(entity.id),
                "name": entity["name"],
                "type": entity.get("type", ""),
                "description": entity.get("description", ""),
                "properties": dict(entity.get("properties", {}))
            }
        
        return entities
```

## ğŸ’¡ å­˜å‚¨ç»„åˆç­–ç•¥ (2025ç‰ˆ)

### æ™ºèƒ½å­˜å‚¨è·¯ç”±
```python
class StorageRouter:
    """2025å¹´æ™ºèƒ½å­˜å‚¨è·¯ç”±å™¨"""
    
    def __init__(self):
        self.kv_storage: Optional[RedisKVStorage] = None
        self.vector_storage: Optional[PGVectorStorage] = None
        self.graph_storage: Optional[Neo4JStorage] = None
        
    async def initialize_all(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–æ‰€æœ‰å­˜å‚¨"""
        # å¹¶è¡Œåˆå§‹åŒ–å­˜å‚¨
        tasks = []
        
        if config.get("redis"):
            self.kv_storage = RedisKVStorage(config["redis"])
            tasks.append(self.kv_storage.initialize())
        
        if config.get("postgres"):
            self.vector_storage = PGVectorStorage(config["postgres"])
            tasks.append(self.vector_storage.initialize())
        
        if config.get("neo4j"):
            self.graph_storage = Neo4JStorage(config["neo4j"])
            tasks.append(self.graph_storage.initialize())
        
        # å¹¶è¡Œåˆå§‹åŒ–
        await asyncio.gather(*tasks)
    
    async def store_document(self, doc_id: str, content: str, embedding: np.ndarray, metadata: Dict[str, Any]):
        """æ™ºèƒ½æ–‡æ¡£å­˜å‚¨"""
        tasks = []
        
        # KVå­˜å‚¨ï¼šç¼“å­˜æ–‡æ¡£å†…å®¹
        if self.kv_storage:
            tasks.append(self.kv_storage.set(f"doc:{doc_id}", {
                "content": content,
                "metadata": metadata
            }, ex=3600))  # 1å°æ—¶ç¼“å­˜
        
        # å‘é‡å­˜å‚¨ï¼šå­˜å‚¨åµŒå…¥å‘é‡
        if self.vector_storage:
            tasks.append(self.vector_storage.upsert_embeddings([
                (content, embedding, metadata)
            ]))
        
        # å¹¶è¡Œå­˜å‚¨
        results = await asyncio.gather(*tasks, return_exceptions=True)
        return all(isinstance(r, bool) and r for r in results if not isinstance(r, Exception))
    
    async def hybrid_search(self, query_vector: np.ndarray, query_text: str, top_k: int = 10) -> Dict[str, Any]:
        """æ··åˆæœç´¢ï¼šå‘é‡ + å›¾è°±"""
        # å¹¶è¡Œæ‰§è¡Œå‘é‡æœç´¢å’Œå›¾æœç´¢
        tasks = []
        
        if self.vector_storage:
            tasks.append(self.vector_storage.similarity_search(query_vector, top_k))
        
        if self.graph_storage:
            # ä»æŸ¥è¯¢æ–‡æœ¬æå–å®ä½“åç§°è¿›è¡Œå›¾æœç´¢
            entities = self._extract_entities(query_text)
            if entities:
                tasks.append(self.graph_storage.get_subgraph(entities, max_depth=2))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return {
            "vector_results": results[0] if len(results) > 0 and not isinstance(results[0], Exception) else [],
            "graph_results": results[1] if len(results) > 1 and not isinstance(results[1], Exception) else {"nodes": [], "edges": []}
        }
    
    def _extract_entities(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬æå–å®ä½“åç§°ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # å®é™…å®ç°ä¸­åº”è¯¥ä½¿ç”¨NERæ¨¡å‹
        return []
```

## âš ï¸ æ³¨æ„äº‹é¡¹ (2025ç‰ˆ)

1. **è¿æ¥æ± ç®¡ç†**: åˆç†é…ç½®è¿æ¥æ± å¤§å°ï¼Œé¿å…èµ„æºæµªè´¹
2. **å¼‚æ­¥æ“ä½œ**: æ‰€æœ‰I/Oæ“ä½œä½¿ç”¨async/awaitï¼Œé¿å…é˜»å¡
3. **æ‰¹é‡å¤„ç†**: ä¼˜å…ˆä½¿ç”¨æ‰¹é‡æ“ä½œæå‡æ€§èƒ½
4. **é”™è¯¯å¤„ç†**: å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œé‡è¯•æœºåˆ¶
5. **ç›‘æ§å‘Šè­¦**: å®æ—¶ç›‘æ§å­˜å‚¨æ€§èƒ½å’Œå¥åº·çŠ¶æ€
6. **æ•°æ®ä¸€è‡´æ€§**: å¤„ç†å¥½åˆ†å¸ƒå¼å­˜å‚¨çš„ä¸€è‡´æ€§é—®é¢˜
7. **å¤‡ä»½æ¢å¤**: å®šæœŸå¤‡ä»½é‡è¦æ•°æ®
8. **ç´¢å¼•ä¼˜åŒ–**: æ ¹æ®æŸ¥è¯¢æ¨¡å¼ä¼˜åŒ–æ•°æ®åº“ç´¢å¼•
9. **ç¼“å­˜ç­–ç•¥**: åˆç†ä½¿ç”¨ç¼“å­˜å‡å°‘æ•°æ®åº“å‹åŠ›
10. **å®‰å…¨è€ƒè™‘**: æ•°æ®åŠ å¯†ã€è®¿é—®æ§åˆ¶ã€SQLæ³¨å…¥é˜²æŠ¤

è¿™äº›2025å¹´çš„å­˜å‚¨åç«¯æœ€ä½³å®è·µå°†ä¸ºLightRAGæä¾›é«˜æ€§èƒ½ã€é«˜å¯ç”¨çš„æ•°æ®å­˜å‚¨è§£å†³æ–¹æ¡ˆï¼
# LightRAG å­˜å‚¨åç«¯å¼€å‘è§„åˆ™ (2025ç‰ˆ)

## ğŸ—„ï¸ å­˜å‚¨æ¶æ„æ¦‚è§ˆ

LightRAGé‡‡ç”¨åˆ†å±‚å­˜å‚¨æ¶æ„ï¼Œæ”¯æŒå¤šç§å­˜å‚¨åç«¯çš„æ— ç¼åˆ‡æ¢ã€‚æ ¹æ®ç”¨æˆ·é…ç½®ï¼Œå½“å‰ä½¿ç”¨ï¼š
- **KVå­˜å‚¨**: RedisKVStorage (é”®å€¼å¯¹å­˜å‚¨)
- **æ–‡æ¡£çŠ¶æ€**: PGDocStatusStorage (PostgreSQL)
- **å›¾å­˜å‚¨**: Neo4JStorage (Neo4jå›¾æ•°æ®åº“) 
- **å‘é‡å­˜å‚¨**: PGVectorStorage (PostgreSQL + pgvector)

æ”¯æŒä»æœ¬åœ°å¼€å‘åˆ°ç”Ÿäº§ç¯å¢ƒçš„å¹³æ»‘è¿ç§»ã€‚

## ğŸ“‚ å­˜å‚¨æ¨¡å—ç»“æ„

```
lightrag/kg/
â”œâ”€â”€ shared_storage.py           # å­˜å‚¨æ¥å£å®šä¹‰å’Œç®¡ç†å™¨
â”œâ”€â”€ json_kv_impl.py            # JSONæœ¬åœ°å­˜å‚¨ (å¼€å‘)
â”œâ”€â”€ json_doc_status_impl.py    # JSONæ–‡æ¡£çŠ¶æ€å­˜å‚¨
â”œâ”€â”€ redis_impl.py              # Rediså®ç° (ç”Ÿäº§æ¨è)
â”œâ”€â”€ postgres_impl.py           # PostgreSQLå®ç° (å‘é‡+æ–‡æ¡£)
â”œâ”€â”€ neo4j_impl.py              # Neo4jå®ç° (å›¾å­˜å‚¨)
â”œâ”€â”€ mongo_impl.py              # MongoDBå®ç°
â”œâ”€â”€ milvus_impl.py             # Milvuså‘é‡æ•°æ®åº“
â”œâ”€â”€ qdrant_impl.py             # Qdrantå‘é‡æ•°æ®åº“
â”œâ”€â”€ faiss_impl.py              # Faisså‘é‡ç´¢å¼• (æœ¬åœ°)
â”œâ”€â”€ memgraph_impl.py           # Memgraphå›¾æ•°æ®åº“
â””â”€â”€ nano_vector_db_impl.py     # è½»é‡çº§å‘é‡æ•°æ®åº“
```

## ğŸ”§ 2025å¹´å­˜å‚¨æ¥å£è§„èŒƒ

### å¼‚æ­¥å­˜å‚¨åŸºç±»è®¾è®¡
```python
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional, Union, AsyncGenerator
import asyncio
import logging
from contextlib import asynccontextmanager

logger = logging.getLogger(__name__)

class BaseStorageManager(ABC):
    """2025å¹´å¼‚æ­¥å­˜å‚¨ç®¡ç†å™¨åŸºç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.is_initialized = False
        self.connection_pool: Optional[Any] = None
        
    async def __aenter__(self):
        await self.initialize()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.close()
    
    @abstractmethod
    async def initialize(self) -> None:
        """åˆå§‹åŒ–å­˜å‚¨è¿æ¥"""
        pass
    
    @abstractmethod
    async def close(self) -> None:
        """å…³é—­å­˜å‚¨è¿æ¥"""
        pass
    
    @abstractmethod
    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        pass
    
    # å¼‚æ­¥æ‰¹é‡æ“ä½œæ¥å£
    @abstractmethod
    async def batch_get(self, keys: List[str]) -> Dict[str, Any]:
        """æ‰¹é‡è·å–æ•°æ®"""
        pass
    
    @abstractmethod
    async def batch_set(self, data: Dict[str, Any]) -> bool:
        """æ‰¹é‡è®¾ç½®æ•°æ®"""
        pass
    
    @abstractmethod
    async def batch_delete(self, keys: List[str]) -> bool:
        """æ‰¹é‡åˆ é™¤æ•°æ®"""
        pass
    
    # æµå¼æ“ä½œæ¥å£ (2025æ–°å¢)
    @abstractmethod
    async def stream_keys(self, pattern: str = "*") -> AsyncGenerator[str, None]:
        """æµå¼è·å–é”®å"""
        pass
    
    @abstractmethod
    async def stream_values(self, keys: List[str]) -> AsyncGenerator[Any, None]:
        """æµå¼è·å–å€¼"""
        pass
```

## ğŸ”´ Rediså¼‚æ­¥å®ç° (2025ä¼˜åŒ–ç‰ˆ)

### Redisè¿æ¥æ± ç®¡ç†
```python
import redis.asyncio as redis
import json
import asyncio
from typing import Optional, Dict, Any, List
import time

class RedisKVStorage(BaseStorageManager):
    """2025å¹´Rediså¼‚æ­¥å­˜å‚¨å®ç°"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.pool: Optional[redis.ConnectionPool] = None
        self.client: Optional[redis.Redis] = None
        self.cluster_mode = config.get("cluster_mode", False)
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "operations": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "errors": 0,
            "avg_response_time": 0.0
        }
    
    async def initialize(self) -> None:
        """åˆå§‹åŒ–Redisè¿æ¥æ±  - 2025æœ€ä½³é…ç½®"""
        try:
            if self.cluster_mode:
                # Redis Clusteræ¨¡å¼
                from redis.asyncio.cluster import RedisCluster
                self.client = RedisCluster(
                    startup_nodes=[
                        {"host": node["host"], "port": node["port"]}
                        for node in self.config.get("cluster_nodes", [])
                    ],
                    decode_responses=True,
                    skip_full_coverage_check=True,
                    max_connections_per_node=20,
                    retry_on_timeout=True,
                    health_check_interval=30
                )
            else:
                # å•æœºæ¨¡å¼è¿æ¥æ± 
                self.pool = redis.ConnectionPool.from_url(
                    self.config.get("url", "redis://localhost:6379/0"),
                    max_connections=50,           # è¿æ¥æ± å¤§å°
                    socket_connect_timeout=5,     # è¿æ¥è¶…æ—¶
                    socket_timeout=30,            # æ“ä½œè¶…æ—¶
                    retry_on_timeout=True,        # è¶…æ—¶é‡è¯•
                    health_check_interval=30,     # å¥åº·æ£€æŸ¥é—´éš”
                    encoding='utf-8',
                    decode_responses=True
                )
                self.client = redis.Redis.from_pool(self.pool)
            
            # æµ‹è¯•è¿æ¥
            await self.client.ping()
            self.is_initialized = True
            logger.info("Redisè¿æ¥åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"Redisè¿æ¥åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def close(self) -> None:
        """æ­£ç¡®å…³é—­Redisè¿æ¥"""
        if self.client:
            await self.client.aclose()
        if self.pool:
            await self.pool.aclose()
        self.is_initialized = False
        logger.info("Redisè¿æ¥å·²å…³é—­")
    
    async def health_check(self) -> bool:
        """Rediså¥åº·æ£€æŸ¥"""
        try:
            start_time = time.time()
            result = await asyncio.wait_for(self.client.ping(), timeout=5.0)
            response_time = time.time() - start_time
            
            # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
            self.stats["avg_response_time"] = (
                self.stats["avg_response_time"] * 0.9 + response_time * 0.1
            )
            
            return result
        except Exception as e:
            logger.error(f"Rediså¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            self.stats["errors"] += 1
            return False
    
    # é«˜æ€§èƒ½å¼‚æ­¥æ“ä½œå®ç°
    async def get(self, key: str) -> Optional[Any]:
        """å¼‚æ­¥è·å–å•ä¸ªé”®å€¼"""
        start_time = time.time()
        try:
            self.stats["operations"] += 1
            value = await self.client.get(key)
            
            if value is not None:
                self.stats["cache_hits"] += 1
                # å°è¯•è§£æJSONï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›åŸå§‹å­—ç¬¦ä¸²
                try:
                    return json.loads(value)
                except (json.JSONDecodeError, TypeError):
                    return value
            else:
                self.stats["cache_misses"] += 1
                return None
                
        except Exception as e:
            self.stats["errors"] += 1
            logger.error(f"Redis GETé”™è¯¯ {key}: {e}")
            return None
        finally:
            # æ›´æ–°å“åº”æ—¶é—´ç»Ÿè®¡
            response_time = time.time() - start_time
            self.stats["avg_response_time"] = (
                self.stats["avg_response_time"] * 0.9 + response_time * 0.1
            )
    
    async def set(self, key: str, value: Any, ex: Optional[int] = None) -> bool:
        """å¼‚æ­¥è®¾ç½®é”®å€¼å¯¹"""
        try:
            self.stats["operations"] += 1
            
            # åºåˆ—åŒ–å€¼
            if isinstance(value, (dict, list)):
                serialized_value = json.dumps(value, ensure_ascii=False)
            else:
                serialized_value = str(value)
            
            # è®¾ç½®è¿‡æœŸæ—¶é—´
            if ex is not None:
                result = await self.client.set(key, serialized_value, ex=ex)
            else:
                result = await self.client.set(key, serialized_value)
            
            return bool(result)
            
        except Exception as e:
            self.stats["errors"] += 1
            logger.error(f"Redis SETé”™è¯¯ {key}: {e}")
            return False
    
    async def batch_get(self, keys: List[str]) -> Dict[str, Any]:
        """æ‰¹é‡è·å– - 2025ä¼˜åŒ–ç‰ˆ"""
        if not keys:
            return {}
        
        try:
            self.stats["operations"] += len(keys)
            
            # ä½¿ç”¨mgetæ‰¹é‡è·å–
            values = await self.client.mget(keys)
            
            result = {}
            for key, value in zip(keys, values):
                if value is not None:
                    self.stats["cache_hits"] += 1
                    try:
                        result[key] = json.loads(value)
                    except (json.JSONDecodeError, TypeError):
                        result[key] = value
                else:
                    self.stats["cache_misses"] += 1
            
            return result
            
        except Exception as e:
            self.stats["errors"] += len(keys)
            logger.error(f"Redisæ‰¹é‡GETé”™è¯¯: {e}")
            return {}
    
    async def batch_set(self, data: Dict[str, Any], ex: Optional[int] = None) -> bool:
        """æ‰¹é‡è®¾ç½® - ä½¿ç”¨pipelineä¼˜åŒ–"""
        if not data:
            return True
        
        try:
            self.stats["operations"] += len(data)
            
            # ä½¿ç”¨pipelineæ‰¹é‡è®¾ç½®
            async with self.client.pipeline(transaction=True) as pipe:
                for key, value in data.items():
                    # åºåˆ—åŒ–å€¼
                    if isinstance(value, (dict, list)):
                        serialized_value = json.dumps(value, ensure_ascii=False)
                    else:
                        serialized_value = str(value)
                    
                    if ex is not None:
                        pipe.set(key, serialized_value, ex=ex)
                    else:
                        pipe.set(key, serialized_value)
                
                results = await pipe.execute()
                return all(results)
                
        except Exception as e:
            self.stats["errors"] += len(data)
            logger.error(f"Redisæ‰¹é‡SETé”™è¯¯: {e}")
            return False
    
    async def batch_delete(self, keys: List[str]) -> bool:
        """æ‰¹é‡åˆ é™¤"""
        if not keys:
            return True
        
        try:
            self.stats["operations"] += len(keys)
            deleted_count = await self.client.delete(*keys)
            return deleted_count > 0
            
        except Exception as e:
            self.stats["errors"] += len(keys)
            logger.error(f"Redisæ‰¹é‡DELETEé”™è¯¯: {e}")
            return False
    
    async def stream_keys(self, pattern: str = "*") -> AsyncGenerator[str, None]:
        """æµå¼è·å–é”®å - ä½¿ç”¨SCANé¿å…é˜»å¡"""
        try:
            cursor = 0
            while True:
                cursor, keys = await self.client.scan(
                    cursor=cursor, 
                    match=pattern, 
                    count=100  # æ¯æ¬¡æ‰«æ100ä¸ªé”®
                )
                
                for key in keys:
                    yield key
                
                if cursor == 0:  # æ‰«æå®Œæ¯•
                    break
                    
                # é¿å…è¿‡å¿«è¿­ä»£ï¼Œç»™å…¶ä»–æ“ä½œè®©å‡ºæ—¶é—´
                await asyncio.sleep(0.001)
                
        except Exception as e:
            logger.error(f"Redis SCANé”™è¯¯: {e}")
    
    async def stream_values(self, keys: List[str]) -> AsyncGenerator[Any, None]:
        """æµå¼è·å–å€¼ - åˆ†æ‰¹å¤„ç†å¤§é‡é”®"""
        batch_size = 100
        
        for i in range(0, len(keys), batch_size):
            batch_keys = keys[i:i + batch_size]
            batch_values = await self.batch_get(batch_keys)
            
            for key in batch_keys:
                yield batch_values.get(key)
            
            # é¿å…é˜»å¡äº‹ä»¶å¾ªç¯
            await asyncio.sleep(0.001)
    
    # 2025æ–°å¢ï¼šæ€§èƒ½ç›‘æ§æ–¹æ³•
    async def get_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        total_ops = self.stats["operations"]
        hit_rate = (
            self.stats["cache_hits"] / total_ops * 100 
            if total_ops > 0 else 0
        )
        
        return {
            **self.stats,
            "cache_hit_rate": f"{hit_rate:.2f}%",
            "error_rate": f"{self.stats['errors'] / total_ops * 100:.2f}%" if total_ops > 0 else "0%",
            "avg_response_time_ms": f"{self.stats['avg_response_time'] * 1000:.2f}ms"
        }
    
    async def reset_stats(self):
        """é‡ç½®æ€§èƒ½ç»Ÿè®¡"""
        self.stats = {
            "operations": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "errors": 0,
            "avg_response_time": 0.0
        }
```

## ğŸ˜ PostgreSQLå¼‚æ­¥å®ç° (2025ä¼˜åŒ–ç‰ˆ)

### PostgreSQL + pgvectoræœ€ä½³å®è·µ
```python
import asyncpg
import numpy as np
import json
from typing import List, Dict, Any, Optional, Tuple
import time

class PGVectorStorage(BaseStorageManager):
    """2025å¹´PostgreSQL + pgvectorå¼‚æ­¥å®ç°"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.pool: Optional[asyncpg.Pool] = None
        self.vector_dim = config.get("vector_dim", 1024)
        
    async def initialize(self) -> None:
        """åˆå§‹åŒ–PostgreSQLè¿æ¥æ± """
        try:
            # 2025å¹´ä¼˜åŒ–çš„è¿æ¥æ± é…ç½®
            self.pool = await asyncpg.create_pool(
                host=self.config.get("host", "localhost"),
                port=self.config.get("port", 5432),
                user=self.config.get("user", "postgres"),
                password=self.config.get("password"),
                database=self.config.get("database", "lightrag_db"),
                min_size=5,                    # æœ€å°è¿æ¥æ•°
                max_size=50,                   # æœ€å¤§è¿æ¥æ•°
                command_timeout=60,            # å‘½ä»¤è¶…æ—¶
                server_settings={
                    'jit': 'off',              # ç¦ç”¨JITæå‡å°æŸ¥è¯¢æ€§èƒ½
                    'shared_preload_libraries': 'vector,pg_stat_statements',
                    'max_connections': '200',
                    'work_mem': '256MB',       # æå‡æ’åºæ€§èƒ½
                    'maintenance_work_mem': '1GB',  # æå‡ç´¢å¼•æ„å»ºæ€§èƒ½
                    'effective_cache_size': '4GB',  # ç¼“å­˜å¤§å°
                    'random_page_cost': '1.1',     # SSDä¼˜åŒ–
                    'effective_io_concurrency': '200'  # å¹¶å‘I/O
                }
            )
            
            await self._create_extensions_and_tables()
            self.is_initialized = True
            logger.info("PostgreSQL + pgvectorè¿æ¥åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"PostgreSQLè¿æ¥åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def _create_extensions_and_tables(self):
        """åˆ›å»ºæ‰©å±•å’Œè¡¨ç»“æ„"""
        async with self.pool.acquire() as conn:
            # åˆ›å»ºæ‰©å±•
            await conn.execute("CREATE EXTENSION IF NOT EXISTS vector;")
            await conn.execute("CREATE EXTENSION IF NOT EXISTS pg_stat_statements;")
            
            # åˆ›å»ºå‘é‡è¡¨
            await conn.execute(f"""
                CREATE TABLE IF NOT EXISTS embeddings (
                    id SERIAL PRIMARY KEY,
                    content TEXT NOT NULL,
                    embedding vector({self.vector_dim}),
                    metadata JSONB DEFAULT '{{}}',
                    created_at TIMESTAMP DEFAULT NOW(),
                    updated_at TIMESTAMP DEFAULT NOW()
                );
            """)
            
            # åˆ›å»ºç´¢å¼• - 2025å¹´ä¼˜åŒ–é…ç½®
            await conn.execute("""
                CREATE INDEX IF NOT EXISTS embeddings_vector_cosine_idx 
                ON embeddings USING ivfflat (embedding vector_cosine_ops) 
                WITH (lists = 1000);
            """)
            
            await conn.execute("""
                CREATE INDEX IF NOT EXISTS embeddings_metadata_idx 
                ON embeddings USING GIN (metadata);
            """)
            
            await conn.execute("""
                CREATE INDEX IF NOT EXISTS embeddings_created_at_idx 
                ON embeddings (created_at DESC);
            """)
            
            # åˆ›å»ºæ›´æ–°æ—¶é—´è§¦å‘å™¨
            await conn.execute("""
                CREATE OR REPLACE FUNCTION update_updated_at_column()
                RETURNS TRIGGER AS $$
                BEGIN
                    NEW.updated_at = NOW();
                    RETURN NEW;
                END;
                $$ language 'plpgsql';
            """)
            
            await conn.execute("""
                DROP TRIGGER IF EXISTS update_embeddings_updated_at ON embeddings;
                CREATE TRIGGER update_embeddings_updated_at 
                BEFORE UPDATE ON embeddings 
                FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
            """)
    
    async def close(self) -> None:
        """å…³é—­è¿æ¥æ± """
        if self.pool:
            await self.pool.close()
        self.is_initialized = False
        logger.info("PostgreSQLè¿æ¥å·²å…³é—­")
    
    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        try:
            async with self.pool.acquire() as conn:
                result = await conn.fetchval("SELECT 1")
                return result == 1
        except Exception as e:
            logger.error(f"PostgreSQLå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    async def upsert_embeddings(
        self, 
        data: List[Tuple[str, np.ndarray, Dict[str, Any]]]
    ) -> bool:
        """æ‰¹é‡æ’å…¥/æ›´æ–°å‘é‡æ•°æ® - 2025ä¼˜åŒ–ç‰ˆ"""
        if not data:
            return True
        
        try:
            async with self.pool.acquire() as conn:
                # ä½¿ç”¨COPYè¿›è¡Œé«˜æ€§èƒ½æ‰¹é‡æ’å…¥
                records = []
                for content, embedding, metadata in data:
                    records.append((
                        content,
                        embedding.tolist(),  # è½¬æ¢ä¸ºåˆ—è¡¨
                        json.dumps(metadata)
                    ))
                
                # ä½¿ç”¨ON CONFLICTè¿›è¡Œupsert
                await conn.executemany("""
                    INSERT INTO embeddings (content, embedding, metadata)
                    VALUES ($1, $2::vector, $3::jsonb)
                    ON CONFLICT (content) DO UPDATE SET
                        embedding = EXCLUDED.embedding,
                        metadata = EXCLUDED.metadata,
                        updated_at = NOW()
                """, records)
                
                return True
                
        except Exception as e:
            logger.error(f"PostgreSQLæ‰¹é‡upserté”™è¯¯: {e}")
            return False
    
    async def similarity_search(
        self, 
        query_vector: np.ndarray, 
        top_k: int = 10,
        threshold: float = 0.7,
        metadata_filter: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """å‘é‡ç›¸ä¼¼åº¦æœç´¢ - 2025ä¼˜åŒ–ç‰ˆ"""
        try:
            async with self.pool.acquire() as conn:
                # æ„å»ºæŸ¥è¯¢æ¡ä»¶
                where_clause = ""
                params = [query_vector.tolist(), threshold, top_k]
                
                if metadata_filter:
                    conditions = []
                    for key, value in metadata_filter.items():
                        conditions.append(f"metadata ->> '{key}' = ${len(params) + 1}")
                        params.append(str(value))
                    
                    if conditions:
                        where_clause = f"AND {' AND '.join(conditions)}"
                
                # æ‰§è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢
                query = f"""
                    SELECT 
                        id, 
                        content, 
                        metadata, 
                        1 - (embedding <=> $1::vector) as similarity,
                        created_at
                    FROM embeddings
                    WHERE 1 - (embedding <=> $1::vector) > $2
                    {where_clause}
                    ORDER BY embedding <=> $1::vector
                    LIMIT $3
                """
                
                rows = await conn.fetch(query, *params)
                
                return [
                    {
                        "id": row["id"],
                        "content": row["content"],
                        "metadata": row["metadata"],
                        "similarity": float(row["similarity"]),
                        "created_at": row["created_at"].isoformat()
                    }
                    for row in rows
                ]
                
        except Exception as e:
            logger.error(f"PostgreSQLå‘é‡æœç´¢é”™è¯¯: {e}")
            return []
    
    async def get_collection_stats(self) -> Dict[str, Any]:
        """è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯"""
        try:
            async with self.pool.acquire() as conn:
                stats = await conn.fetchrow("""
                    SELECT 
                        COUNT(*) as total_vectors,
                        AVG(ARRAY_LENGTH(embedding::float[], 1)) as avg_dimension,
                        MIN(created_at) as oldest_record,
                        MAX(created_at) as newest_record,
                        pg_size_pretty(pg_total_relation_size('embeddings')) as table_size
                    FROM embeddings
                """)
                
                return dict(stats) if stats else {}
                
        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯é”™è¯¯: {e}")
            return {}
    
    # å®ç°åŸºç±»æ–¹æ³•
    async def batch_get(self, keys: List[str]) -> Dict[str, Any]:
        """æ‰¹é‡è·å–æ–‡æ¡£"""
        try:
            async with self.pool.acquire() as conn:
                rows = await conn.fetch("""
                    SELECT content, metadata, embedding 
                    FROM embeddings 
                    WHERE content = ANY($1::text[])
                """, keys)
                
                return {
                    row["content"]: {
                        "metadata": row["metadata"],
                        "embedding": row["embedding"]
                    }
                    for row in rows
                }
                
        except Exception as e:
            logger.error(f"PostgreSQLæ‰¹é‡è·å–é”™è¯¯: {e}")
            return {}
    
    async def batch_set(self, data: Dict[str, Any]) -> bool:
        """æ‰¹é‡è®¾ç½®æ–‡æ¡£"""
        embeddings_data = [
            (key, np.array(value["embedding"]), value.get("metadata", {}))
            for key, value in data.items()
            if "embedding" in value
        ]
        return await self.upsert_embeddings(embeddings_data)
    
    async def batch_delete(self, keys: List[str]) -> bool:
        """æ‰¹é‡åˆ é™¤æ–‡æ¡£"""
        try:
            async with self.pool.acquire() as conn:
                deleted_count = await conn.execute("""
                    DELETE FROM embeddings 
                    WHERE content = ANY($1::text[])
                """, keys)
                return int(deleted_count.split()[-1]) > 0
                
        except Exception as e:
            logger.error(f"PostgreSQLæ‰¹é‡åˆ é™¤é”™è¯¯: {e}")
            return False
    
    async def stream_keys(self, pattern: str = "*") -> AsyncGenerator[str, None]:
        """æµå¼è·å–å†…å®¹é”®"""
        try:
            async with self.pool.acquire() as conn:
                async with conn.transaction():
                    # ä½¿ç”¨æœåŠ¡å™¨ç«¯æ¸¸æ ‡é¿å…å†…å­˜é—®é¢˜
                    await conn.execute("""
                        DECLARE content_cursor CURSOR FOR 
                        SELECT content FROM embeddings ORDER BY id
                    """)
                    
                    while True:
                        rows = await conn.fetch("FETCH 100 FROM content_cursor")
                        if not rows:
                            break
                        
                        for row in rows:
                            yield row["content"]
                        
                        await asyncio.sleep(0.001)
                        
        except Exception as e:
            logger.error(f"PostgreSQLæµå¼é”®è·å–é”™è¯¯: {e}")
    
    async def stream_values(self, keys: List[str]) -> AsyncGenerator[Any, None]:
        """æµå¼è·å–å€¼"""
        batch_size = 50
        for i in range(0, len(keys), batch_size):
            batch_keys = keys[i:i + batch_size]
            batch_data = await self.batch_get(batch_keys)
            
            for key in batch_keys:
                yield batch_data.get(key)
            
            await asyncio.sleep(0.001)
```

## ğŸ”— Neo4jå›¾æ•°æ®åº“å®ç° (2025ç‰ˆ)

### å¼‚æ­¥Neo4jæ“ä½œ
```python
from neo4j import AsyncGraphDatabase
from typing import List, Dict, Any, Optional
import asyncio

class Neo4JStorage(BaseStorageManager):
    """2025å¹´Neo4jå¼‚æ­¥å›¾å­˜å‚¨å®ç°"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.driver = None
        self.max_connection_lifetime = config.get("max_connection_lifetime", 3600)
        
    async def initialize(self) -> None:
        """åˆå§‹åŒ–Neo4jè¿æ¥"""
        try:
            self.driver = AsyncGraphDatabase.driver(
                self.config.get("uri", "bolt://localhost:7687"),
                auth=(
                    self.config.get("user", "neo4j"),
                    self.config.get("password", "password")
                ),
                max_connection_lifetime=self.max_connection_lifetime,
                max_connection_pool_size=50,
                connection_acquisition_timeout=60,
                encrypted=self.config.get("encrypted", False)
            )
            
            # æµ‹è¯•è¿æ¥
            await self.driver.verify_connectivity()
            
            # åˆ›å»ºç´¢å¼•
            await self._create_indexes()
            
            self.is_initialized = True
            logger.info("Neo4jè¿æ¥åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"Neo4jè¿æ¥åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def _create_indexes(self):
        """åˆ›å»ºå›¾æ•°æ®åº“ç´¢å¼•"""
        async with self.driver.session() as session:
            # å®ä½“ç´¢å¼•
            await session.run("""
                CREATE INDEX entity_name_idx IF NOT EXISTS 
                FOR (e:Entity) ON (e.name)
            """)
            
            # å…³ç³»ç´¢å¼•
            await session.run("""
                CREATE INDEX relation_type_idx IF NOT EXISTS 
                FOR ()-[r:RELATION]-() ON (r.type)
            """)
    
    async def close(self) -> None:
        """å…³é—­Neo4jè¿æ¥"""
        if self.driver:
            await self.driver.close()
        self.is_initialized = False
        logger.info("Neo4jè¿æ¥å·²å…³é—­")
    
    async def health_check(self) -> bool:
        """Neo4jå¥åº·æ£€æŸ¥"""
        try:
            await self.driver.verify_connectivity()
            return True
        except Exception as e:
            logger.error(f"Neo4jå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    async def upsert_entities(self, entities: List[Dict[str, Any]]) -> bool:
        """æ‰¹é‡æ’å…¥/æ›´æ–°å®ä½“"""
        try:
            async with self.driver.session() as session:
                await session.execute_write(self._upsert_entities_tx, entities)
                return True
        except Exception as e:
            logger.error(f"Neo4jå®ä½“upserté”™è¯¯: {e}")
            return False
    
    @staticmethod
    async def _upsert_entities_tx(tx, entities):
        """å®ä½“upsertäº‹åŠ¡"""
        for entity in entities:
            await tx.run("""
                MERGE (e:Entity {name: $name})
                SET e.description = $description,
                    e.type = $type,
                    e.properties = $properties,
                    e.updated_at = datetime()
            """, 
            name=entity["name"],
            description=entity.get("description", ""),
            type=entity.get("type", ""),
            properties=entity.get("properties", {})
            )
    
    async def upsert_relationships(self, relationships: List[Dict[str, Any]]) -> bool:
        """æ‰¹é‡æ’å…¥/æ›´æ–°å…³ç³»"""
        try:
            async with self.driver.session() as session:
                await session.execute_write(self._upsert_relationships_tx, relationships)
                return True
        except Exception as e:
            logger.error(f"Neo4jå…³ç³»upserté”™è¯¯: {e}")
            return False
    
    @staticmethod
    async def _upsert_relationships_tx(tx, relationships):
        """å…³ç³»upsertäº‹åŠ¡"""
        for rel in relationships:
            await tx.run("""
                MATCH (src:Entity {name: $src_name})
                MATCH (tgt:Entity {name: $tgt_name})
                MERGE (src)-[r:RELATION {type: $rel_type}]->(tgt)
                SET r.description = $description,
                    r.properties = $properties,
                    r.updated_at = datetime()
            """,
            src_name=rel["source"],
            tgt_name=rel["target"],
            rel_type=rel["type"],
            description=rel.get("description", ""),
            properties=rel.get("properties", {})
            )
    
    async def get_subgraph(
        self, 
        entity_names: List[str], 
        max_depth: int = 2
    ) -> Dict[str, Any]:
        """è·å–å­å›¾"""
        try:
            async with self.driver.session() as session:
                result = await session.execute_read(
                    self._get_subgraph_tx, 
                    entity_names, 
                    max_depth
                )
                return result
        except Exception as e:
            logger.error(f"Neo4jå­å›¾æŸ¥è¯¢é”™è¯¯: {e}")
            return {"nodes": [], "edges": []}
    
    @staticmethod
    async def _get_subgraph_tx(tx, entity_names, max_depth):
        """å­å›¾æŸ¥è¯¢äº‹åŠ¡"""
        # æŸ¥è¯¢èŠ‚ç‚¹å’Œå…³ç³»
        result = await tx.run(f"""
            MATCH path = (start:Entity)-[*1..{max_depth}]-(end:Entity)
            WHERE start.name IN $entity_names
            RETURN nodes(path) as nodes, relationships(path) as rels
        """, entity_names=entity_names)
        
        nodes = []
        edges = []
        node_ids = set()
        edge_ids = set()
        
        async for record in result:
            # å¤„ç†èŠ‚ç‚¹
            for node in record["nodes"]:
                if node.id not in node_ids:
                    nodes.append({
                        "id": str(node.id),
                        "name": node["name"],
                        "type": node.get("type", ""),
                        "description": node.get("description", ""),
                        "properties": dict(node.get("properties", {}))
                    })
                    node_ids.add(node.id)
            
            # å¤„ç†å…³ç³»
            for rel in record["rels"]:
                if rel.id not in edge_ids:
                    edges.append({
                        "id": str(rel.id),
                        "source": str(rel.start_node.id),
                        "target": str(rel.end_node.id),
                        "type": rel["type"],
                        "description": rel.get("description", ""),
                        "properties": dict(rel.get("properties", {}))
                    })
                    edge_ids.add(rel.id)
        
        return {"nodes": nodes, "edges": edges}
    
    # å®ç°åŸºç±»æ–¹æ³•
    async def batch_get(self, keys: List[str]) -> Dict[str, Any]:
        """æ‰¹é‡è·å–å®ä½“"""
        try:
            async with self.driver.session() as session:
                result = await session.execute_read(self._batch_get_entities_tx, keys)
                return result
        except Exception as e:
            logger.error(f"Neo4jæ‰¹é‡è·å–é”™è¯¯: {e}")
            return {}
    
    @staticmethod
    async def _batch_get_entities_tx(tx, entity_names):
        """æ‰¹é‡è·å–å®ä½“äº‹åŠ¡"""
        result = await tx.run("""
            MATCH (e:Entity)
            WHERE e.name IN $names
            RETURN e.name as name, e
        """, names=entity_names)
        
        entities = {}
        async for record in result:
            entity = record["e"]
            entities[record["name"]] = {
                "id": str(entity.id),
                "name": entity["name"],
                "type": entity.get("type", ""),
                "description": entity.get("description", ""),
                "properties": dict(entity.get("properties", {}))
            }
        
        return entities
```

## ğŸ’¡ å­˜å‚¨ç»„åˆç­–ç•¥ (2025ç‰ˆ)

### æ™ºèƒ½å­˜å‚¨è·¯ç”±
```python
class StorageRouter:
    """2025å¹´æ™ºèƒ½å­˜å‚¨è·¯ç”±å™¨"""
    
    def __init__(self):
        self.kv_storage: Optional[RedisKVStorage] = None
        self.vector_storage: Optional[PGVectorStorage] = None
        self.graph_storage: Optional[Neo4JStorage] = None
        
    async def initialize_all(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–æ‰€æœ‰å­˜å‚¨"""
        # å¹¶è¡Œåˆå§‹åŒ–å­˜å‚¨
        tasks = []
        
        if config.get("redis"):
            self.kv_storage = RedisKVStorage(config["redis"])
            tasks.append(self.kv_storage.initialize())
        
        if config.get("postgres"):
            self.vector_storage = PGVectorStorage(config["postgres"])
            tasks.append(self.vector_storage.initialize())
        
        if config.get("neo4j"):
            self.graph_storage = Neo4JStorage(config["neo4j"])
            tasks.append(self.graph_storage.initialize())
        
        # å¹¶è¡Œåˆå§‹åŒ–
        await asyncio.gather(*tasks)
    
    async def store_document(self, doc_id: str, content: str, embedding: np.ndarray, metadata: Dict[str, Any]):
        """æ™ºèƒ½æ–‡æ¡£å­˜å‚¨"""
        tasks = []
        
        # KVå­˜å‚¨ï¼šç¼“å­˜æ–‡æ¡£å†…å®¹
        if self.kv_storage:
            tasks.append(self.kv_storage.set(f"doc:{doc_id}", {
                "content": content,
                "metadata": metadata
            }, ex=3600))  # 1å°æ—¶ç¼“å­˜
        
        # å‘é‡å­˜å‚¨ï¼šå­˜å‚¨åµŒå…¥å‘é‡
        if self.vector_storage:
            tasks.append(self.vector_storage.upsert_embeddings([
                (content, embedding, metadata)
            ]))
        
        # å¹¶è¡Œå­˜å‚¨
        results = await asyncio.gather(*tasks, return_exceptions=True)
        return all(isinstance(r, bool) and r for r in results if not isinstance(r, Exception))
    
    async def hybrid_search(self, query_vector: np.ndarray, query_text: str, top_k: int = 10) -> Dict[str, Any]:
        """æ··åˆæœç´¢ï¼šå‘é‡ + å›¾è°±"""
        # å¹¶è¡Œæ‰§è¡Œå‘é‡æœç´¢å’Œå›¾æœç´¢
        tasks = []
        
        if self.vector_storage:
            tasks.append(self.vector_storage.similarity_search(query_vector, top_k))
        
        if self.graph_storage:
            # ä»æŸ¥è¯¢æ–‡æœ¬æå–å®ä½“åç§°è¿›è¡Œå›¾æœç´¢
            entities = self._extract_entities(query_text)
            if entities:
                tasks.append(self.graph_storage.get_subgraph(entities, max_depth=2))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return {
            "vector_results": results[0] if len(results) > 0 and not isinstance(results[0], Exception) else [],
            "graph_results": results[1] if len(results) > 1 and not isinstance(results[1], Exception) else {"nodes": [], "edges": []}
        }
    
    def _extract_entities(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬æå–å®ä½“åç§°ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # å®é™…å®ç°ä¸­åº”è¯¥ä½¿ç”¨NERæ¨¡å‹
        return []
```

## âš ï¸ æ³¨æ„äº‹é¡¹ (2025ç‰ˆ)

1. **è¿æ¥æ± ç®¡ç†**: åˆç†é…ç½®è¿æ¥æ± å¤§å°ï¼Œé¿å…èµ„æºæµªè´¹
2. **å¼‚æ­¥æ“ä½œ**: æ‰€æœ‰I/Oæ“ä½œä½¿ç”¨async/awaitï¼Œé¿å…é˜»å¡
3. **æ‰¹é‡å¤„ç†**: ä¼˜å…ˆä½¿ç”¨æ‰¹é‡æ“ä½œæå‡æ€§èƒ½
4. **é”™è¯¯å¤„ç†**: å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œé‡è¯•æœºåˆ¶
5. **ç›‘æ§å‘Šè­¦**: å®æ—¶ç›‘æ§å­˜å‚¨æ€§èƒ½å’Œå¥åº·çŠ¶æ€
6. **æ•°æ®ä¸€è‡´æ€§**: å¤„ç†å¥½åˆ†å¸ƒå¼å­˜å‚¨çš„ä¸€è‡´æ€§é—®é¢˜
7. **å¤‡ä»½æ¢å¤**: å®šæœŸå¤‡ä»½é‡è¦æ•°æ®
8. **ç´¢å¼•ä¼˜åŒ–**: æ ¹æ®æŸ¥è¯¢æ¨¡å¼ä¼˜åŒ–æ•°æ®åº“ç´¢å¼•
9. **ç¼“å­˜ç­–ç•¥**: åˆç†ä½¿ç”¨ç¼“å­˜å‡å°‘æ•°æ®åº“å‹åŠ›
10. **å®‰å…¨è€ƒè™‘**: æ•°æ®åŠ å¯†ã€è®¿é—®æ§åˆ¶ã€SQLæ³¨å…¥é˜²æŠ¤

è¿™äº›2025å¹´çš„å­˜å‚¨åç«¯æœ€ä½³å®è·µå°†ä¸ºLightRAGæä¾›é«˜æ€§èƒ½ã€é«˜å¯ç”¨çš„æ•°æ®å­˜å‚¨è§£å†³æ–¹æ¡ˆï¼
