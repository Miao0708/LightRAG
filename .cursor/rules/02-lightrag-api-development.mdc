# LightRAG API å¼€å‘è§„åˆ™

## ğŸŒ APIæ¶æ„æ¦‚è§ˆ

LightRAG APIåŸºäºFastAPIæ„å»ºï¼Œæä¾›RESTfulæ¥å£å’ŒOllamaå…¼å®¹æ¥å£ï¼Œæ”¯æŒæ–‡æ¡£ç®¡ç†ã€çŸ¥è¯†å›¾è°±æ“ä½œå’Œå…­ç§æŸ¥è¯¢æ¨¡å¼çš„æ™ºèƒ½é—®ç­”ã€‚

## ğŸ“‚ APIæ¨¡å—ç»“æ„

```
lightrag/api/
â”œâ”€â”€ lightrag_server.py          # ä¸»æœåŠ¡å™¨å…¥å£
â”œâ”€â”€ config.py                   # é…ç½®ç®¡ç†
â”œâ”€â”€ auth.py                     # è®¤è¯ä¸­é—´ä»¶
â”œâ”€â”€ utils_api.py                # APIå·¥å…·å‡½æ•°
â”œâ”€â”€ routers/                    # è·¯ç”±æ¨¡å—
â”‚   â”œâ”€â”€ document_routes.py      # æ–‡æ¡£ç®¡ç†è·¯ç”±
â”‚   â”œâ”€â”€ query_routes.py         # æŸ¥è¯¢ç›¸å…³è·¯ç”±
â”‚   â”œâ”€â”€ graph_routes.py         # å›¾è°±æ“ä½œè·¯ç”±
â”‚   â””â”€â”€ ollama_api.py          # Ollamaå…¼å®¹æ¥å£
â””â”€â”€ webui/                      # é™æ€Web UIæ–‡ä»¶
```

## ğŸ”§ 2025å¹´FastAPIæœ€ä½³å®è·µ

### 1. é«˜æ€§èƒ½å¼‚æ­¥æ¶æ„
```python
import uvloop  # 2025å¹´å¿…å¤‡ï¼š2-4xæ€§èƒ½æå‡
import orjson  # æ›¿ä»£æ ‡å‡†jsonï¼Œ2-3xåºåˆ—åŒ–é€Ÿåº¦
from fastapi import FastAPI
from fastapi.responses import ORJSONResponse
from contextlib import asynccontextmanager

# è®¾ç½®é«˜æ€§èƒ½äº‹ä»¶å¾ªç¯
uvloop.install()

@asynccontextmanager
async def lifespan(app: FastAPI):
    # åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–
    await init_database_pools()
    await init_redis_connection()
    yield
    # åº”ç”¨å…³é—­æ—¶æ¸…ç†
    await cleanup_connections()

app = FastAPI(
    title="LightRAG API 2025",
    version="2.0.0",
    lifespan=lifespan,
    default_response_class=ORJSONResponse  # é«˜æ€§èƒ½JSONå“åº”
)

# 2025æ¨èä¸­é—´ä»¶é…ç½®
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(GZipMiddleware, minimum_size=1000)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

### 2. æ™ºèƒ½æŸ¥è¯¢æ¨¡å¼APIè®¾è®¡
```python
from enum import Enum
from pydantic import BaseModel, Field
from typing import Optional, List
import asyncio

class QueryMode(str, Enum):
    NAIVE = "naive"        # çº¯å‘é‡æ£€ç´¢ (0.8s, 72%å‡†ç¡®ç‡)
    LOCAL = "local"        # å®ä½“é‚»åŸŸæ£€ç´¢ (1.2s, 78%å‡†ç¡®ç‡)  
    GLOBAL = "global"      # å…³ç³»ç½‘ç»œæ£€ç´¢ (2.1s, 82%å‡†ç¡®ç‡)
    HYBRID = "hybrid"      # æ··åˆæ£€ç´¢ (1.8s, 85%å‡†ç¡®ç‡)
    MIX = "mix"           # å›¾è°±+å‘é‡èåˆ (2.5s, 88%å‡†ç¡®ç‡)
    BYPASS = "bypass"      # ç›´æ¥LLMè°ƒç”¨ (æœ€å¿«)

class QueryRequest(BaseModel):
    query: str = Field(..., description="ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬")
    mode: QueryMode = Field(QueryMode.HYBRID, description="æŸ¥è¯¢æ¨¡å¼")
    enable_rerank: bool = Field(True, description="æ˜¯å¦å¯ç”¨é‡æ’åº")
    chunk_top_k: int = Field(10, ge=1, le=50, description="è¿”å›ç»“æœæ•°é‡")
    timeout: float = Field(30.0, ge=1.0, le=300.0, description="æŸ¥è¯¢è¶…æ—¶æ—¶é—´")

@app.post("/query", response_class=ORJSONResponse)
async def enhanced_query(request: QueryRequest):
    """
    å¢å¼ºæŸ¥è¯¢æ¥å£ - æ”¯æŒå…­ç§æŸ¥è¯¢æ¨¡å¼å’Œæ™ºèƒ½å‰ç¼€è¯†åˆ«
    
    æ€§èƒ½åŸºå‡† (2025ç‰ˆ):
    - naive: 0.8s, 72%å‡†ç¡®ç‡, ä½èµ„æºæ¶ˆè€—
    - local: 1.2s, 78%å‡†ç¡®ç‡, ä¸­ç­‰èµ„æºæ¶ˆè€—
    - global: 2.1s, 82%å‡†ç¡®ç‡, ä¸­é«˜èµ„æºæ¶ˆè€—
    - hybrid: 1.8s, 85%å‡†ç¡®ç‡, å¹³è¡¡æ€§èƒ½
    - mix: 2.5s, 88%å‡†ç¡®ç‡, æœ€é«˜å‡†ç¡®ç‡
    - bypass: æœ€å¿«å“åº”, ä¾èµ–LLMçŸ¥è¯†
    """
    
    # æ”¯æŒæŸ¥è¯¢å‰ç¼€è‡ªåŠ¨æ¨¡å¼è¯†åˆ«
    if request.query.startswith("/"):
        parts = request.query.split(maxsplit=1)
        if len(parts) == 2:
            mode_prefix = parts[0][1:]
            if mode_prefix in [mode.value for mode in QueryMode]:
                request.mode = QueryMode(mode_prefix)
                request.query = parts[1]
    
    # å¼‚æ­¥æŸ¥è¯¢å¤„ç†ï¼Œæ”¯æŒè¶…æ—¶æ§åˆ¶
    try:
        result = await asyncio.wait_for(
            lightrag.aquery(
                request.query, 
                param=QueryParam(
                    mode=request.mode.value,
                    enable_rerank=request.enable_rerank,
                    chunk_top_k=request.chunk_top_k
                )
            ),
            timeout=request.timeout
        )
        
        return {
            "result": result,
            "metadata": {
                "mode": request.mode.value,
                "rerank_enabled": request.enable_rerank,
                "processing_time": f"{time.time() - start_time:.2f}s"
            }
        }
    except asyncio.TimeoutError:
        raise HTTPException(
            status_code=408, 
            detail=f"æŸ¥è¯¢è¶…æ—¶ ({request.timeout}s)"
        )
```

### 3. é«˜æ€§èƒ½é‡æ’åºé›†æˆ
```python
from lightrag.rerank import custom_rerank
import httpx
import asyncio

class RerankConfig(BaseModel):
    enabled: bool = True
    model: str = "BAAI/bge-reranker-v2-m3"
    top_n: int = 10
    timeout: float = 30.0

@app.post("/query/rerank")
async def query_with_advanced_rerank(
    request: QueryRequest,
    rerank_config: RerankConfig = Depends()
):
    """é«˜æ€§èƒ½é‡æ’åºæŸ¥è¯¢æ¥å£"""
    
    # åŠ¨æ€é…ç½®é‡æ’åºå‡½æ•°
    async def rerank_func(query: str, documents: list, top_n: int = None, **kwargs):
        if not rerank_config.enabled:
            return documents[:top_n] if top_n else documents
        
        try:
            return await asyncio.wait_for(
                custom_rerank(
                    query=query,
                    documents=documents,
                    model=rerank_config.model,
                    base_url=os.getenv("RERANK_BINDING_HOST"),
                    api_key=os.getenv("RERANK_BINDING_API_KEY"),
                    top_n=top_n or rerank_config.top_n
                ),
                timeout=rerank_config.timeout
            )
        except asyncio.TimeoutError:
            logger.warning(f"é‡æ’åºè¶…æ—¶ï¼Œè¿”å›åŸå§‹ç»“æœ")
            return documents[:top_n] if top_n else documents
    
    # ä½¿ç”¨é‡æ’åºå‡½æ•°æŸ¥è¯¢
    return await lightrag.aquery(
        request.query,
        param=QueryParam(
            mode=request.mode.value,
            enable_rerank=rerank_config.enabled,
            rerank_model_func=rerank_func
        )
    )
```

### 4. æµå¼å“åº”å®ç° (2025ä¼˜åŒ–ç‰ˆ)
```python
from fastapi.responses import StreamingResponse
import json
import time

@app.post("/query/stream")
async def stream_query_enhanced(request: QueryRequest):
    """é«˜æ€§èƒ½æµå¼æŸ¥è¯¢å“åº”"""
    
    async def generate_stream():
        start_time = time.time()
        try:
            # åˆå§‹åŒ–æµå¼å“åº”
            yield f"data: {orjson.dumps({'status': 'started', 'mode': request.mode.value}).decode()}\n\n"
            
            # æµå¼å¤„ç†æŸ¥è¯¢
            chunk_count = 0
            async for chunk in lightrag.aquery_stream(
                request.query,
                param=QueryParam(mode=request.mode.value)
            ):
                chunk_count += 1
                chunk_data = {
                    'chunk': chunk, 
                    'chunk_id': chunk_count,
                    'mode': request.mode.value,
                    'timestamp': time.time()
                }
                yield f"data: {orjson.dumps(chunk_data).decode()}\n\n"
                
                # é¿å…è¿‡å¿«å‘é€ï¼Œç»™å®¢æˆ·ç«¯å¤„ç†æ—¶é—´
                await asyncio.sleep(0.01)
            
            # å®Œæˆå“åº”
            completion_data = {
                'status': 'completed',
                'total_chunks': chunk_count,
                'processing_time': f"{time.time() - start_time:.2f}s"
            }
            yield f"data: {orjson.dumps(completion_data).decode()}\n\n"
            
        except Exception as e:
            error_data = {
                'status': 'error',
                'error': str(e),
                'error_type': type(e).__name__
            }
            yield f"data: {orjson.dumps(error_data).decode()}\n\n"
    
    return StreamingResponse(
        generate_stream(), 
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # Nginxä¼˜åŒ–
        }
    )
```

### 5. æ™ºèƒ½æ¨¡å¼é€‰æ‹©API
```python
import re
from typing import Dict, Callable

class QueryClassifier:
    """2025å¹´æ™ºèƒ½æŸ¥è¯¢åˆ†ç±»å™¨"""
    
    def __init__(self):
        self.rules: Dict[str, Callable[[str], bool]] = {
            QueryMode.BYPASS: self._is_casual_chat,
            QueryMode.GLOBAL: self._is_conceptual_query,
            QueryMode.LOCAL: self._is_entity_query,
            QueryMode.MIX: self._is_complex_reasoning,
            QueryMode.HYBRID: self._is_default_query
        }
    
    def _is_casual_chat(self, query: str) -> bool:
        casual_patterns = [
            r'ä½ å¥½|è°¢è°¢|å†è§|æ—©ä¸Šå¥½|æ™šå®‰',
            r'æ€ä¹ˆæ ·|å¦‚ä½•|å¤©æ°”|å¿ƒæƒ…',
            r'^(hi|hello|thanks|bye)',
        ]
        return any(re.search(pattern, query, re.IGNORECASE) for pattern in casual_patterns)
    
    def _is_conceptual_query(self, query: str) -> bool:
        concept_patterns = [
            r'ä»€ä¹ˆæ˜¯|å®šä¹‰|æ¦‚å¿µ|åŸç†|ç†è®º',
            r'è§£é‡Š|è¯´æ˜|å«ä¹‰|æ„æ€',
            r'å¦‚ä½•ç†è§£|æ€ä¹ˆç†è§£'
        ]
        return any(re.search(pattern, query, re.IGNORECASE) for pattern in concept_patterns)
    
    def _is_entity_query(self, query: str) -> bool:
        entity_patterns = [
            r'è¯¦ç»†ä»‹ç»|å…·ä½“è¯´æ˜|è¯¦æƒ…',
            r'å…³äº.*çš„ä¿¡æ¯|.*çš„è¯¦ç»†èµ„æ–™',
            r'å…·ä½“.*æ˜¯ä»€ä¹ˆ'
        ]
        return any(re.search(pattern, query, re.IGNORECASE) for pattern in entity_patterns)
    
    def _is_complex_reasoning(self, query: str) -> bool:
        reasoning_patterns = [
            r'æ¯”è¾ƒ|å¯¹æ¯”|åˆ†æ|è¯„ä¼°',
            r'åŒºåˆ«|å·®å¼‚|ä¼˜åŠ£|ä¼˜ç¼ºç‚¹',
            r'ä¸ºä»€ä¹ˆ|æ€ä¹ˆä¼š|åŸå› |å½±å“',
            r'å…³ç³»|è”ç³»|ç›¸å…³æ€§'
        ]
        return any(re.search(pattern, query, re.IGNORECASE) for pattern in reasoning_patterns)
    
    def _is_default_query(self, query: str) -> bool:
        return True  # é»˜è®¤æƒ…å†µ
    
    def classify(self, query: str) -> QueryMode:
        """æ™ºèƒ½åˆ†ç±»æŸ¥è¯¢ç±»å‹"""
        for mode, rule in self.rules.items():
            if rule(query):
                return mode
        return QueryMode.HYBRID

# å…¨å±€åˆ†ç±»å™¨å®ä¾‹
query_classifier = QueryClassifier()

@app.post("/query/auto")
async def auto_query_enhanced(request: QueryRequest):
    """AIå¢å¼ºçš„æ™ºèƒ½æŸ¥è¯¢æ¥å£"""
    
    # è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æŸ¥è¯¢æ¨¡å¼
    if request.mode == QueryMode.HYBRID:  # å¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šæ¨¡å¼
        suggested_mode = query_classifier.classify(request.query)
        request.mode = suggested_mode
    
    # åŸºäºæ¨¡å¼è‡ªåŠ¨è°ƒæ•´å‚æ•°
    mode_configs = {
        QueryMode.NAIVE: {"chunk_top_k": 5, "enable_rerank": False},
        QueryMode.LOCAL: {"chunk_top_k": 8, "enable_rerank": True},
        QueryMode.GLOBAL: {"chunk_top_k": 10, "enable_rerank": True},
        QueryMode.HYBRID: {"chunk_top_k": 10, "enable_rerank": True},
        QueryMode.MIX: {"chunk_top_k": 15, "enable_rerank": True},
        QueryMode.BYPASS: {"chunk_top_k": 0, "enable_rerank": False}
    }
    
    config = mode_configs.get(request.mode, mode_configs[QueryMode.HYBRID])
    for key, value in config.items():
        if not hasattr(request, key) or getattr(request, key) is None:
            setattr(request, key, value)
    
    return await lightrag.aquery(
        request.query,
        param=QueryParam(
            mode=request.mode.value,
            enable_rerank=request.enable_rerank,
            chunk_top_k=request.chunk_top_k
        )
    )
```

## ğŸ”— Ollamaå…¼å®¹æ¥å£ (2025å¢å¼ºç‰ˆ)

### å®Œæ•´Ollama APIå…¼å®¹
```python
from typing import List, Optional, Any
import uuid

class OllamaMessage(BaseModel):
    role: str
    content: str

class OllamaChatRequest(BaseModel):
    model: str
    messages: List[OllamaMessage]
    stream: bool = False
    options: Optional[dict] = None

class OllamaGenerateRequest(BaseModel):
    model: str
    prompt: str
    stream: bool = False
    options: Optional[dict] = None

@app.post("/api/chat")
async def ollama_chat_compatible(request: OllamaChatRequest):
    """å®Œæ•´çš„Ollama Chat APIå…¼å®¹æ€§"""
    
    # æå–æŸ¥è¯¢å†…å®¹
    query = request.messages[-1].content if request.messages else ""
    
    # LightRAGæ¨¡å‹æ˜ å°„
    lightrag_models = {
        "lightrag:naive": QueryMode.NAIVE,
        "lightrag:local": QueryMode.LOCAL,
        "lightrag:global": QueryMode.GLOBAL,
        "lightrag:hybrid": QueryMode.HYBRID,
        "lightrag:mix": QueryMode.MIX,
        "lightrag:bypass": QueryMode.BYPASS
    }
    
    mode = lightrag_models.get(request.model, QueryMode.HYBRID)
    
    # æ„å»ºå¯¹è¯å†å²
    history_messages = [
        {"role": msg.role, "content": msg.content} 
        for msg in request.messages[:-1]
    ] if len(request.messages) > 1 else None
    
    # æ‰§è¡ŒLightRAGæŸ¥è¯¢
    if request.stream:
        return StreamingResponse(
            stream_ollama_response(query, mode, history_messages),
            media_type="application/x-ndjson"
        )
    else:
        response = await lightrag.aquery(
            query,
            param=QueryParam(
                mode=mode.value,
                enable_rerank=True,
                history_messages=history_messages
            )
        )
        
        return {
            "model": request.model,
            "created_at": time.time(),
            "message": {
                "role": "assistant",
                "content": response
            },
            "done": True,
            "total_duration": int(time.time() * 1e9),
            "load_duration": 0,
            "prompt_eval_count": len(query.split()),
            "eval_count": len(response.split()),
            "eval_duration": int(time.time() * 1e9)
        }

async def stream_ollama_response(query: str, mode: QueryMode, history: List[dict]):
    """Ollamaæµå¼å“åº”æ ¼å¼"""
    async for chunk in lightrag.aquery_stream(
        query,
        param=QueryParam(
            mode=mode.value,
            history_messages=history
        )
    ):
        ollama_chunk = {
            "model": f"lightrag:{mode.value}",
            "created_at": time.time(),
            "message": {
                "role": "assistant",
                "content": chunk
            },
            "done": False
        }
        yield f"{orjson.dumps(ollama_chunk).decode()}\n"
    
    # ç»“æŸæ ‡è®°
    final_chunk = {
        "model": f"lightrag:{mode.value}",
        "created_at": time.time(),
        "message": {"role": "assistant", "content": ""},
        "done": True
    }
    yield f"{orjson.dumps(final_chunk).decode()}\n"

@app.get("/api/tags")
async def ollama_list_models():
    """Ollamaæ¨¡å‹åˆ—è¡¨å…¼å®¹"""
    return {
        "models": [
            {
                "name": f"lightrag:{mode.value}",
                "modified_at": "2025-01-01T00:00:00Z",
                "size": 1000000,
                "digest": f"sha256:{'0' * 64}",
                "details": {
                    "format": "lightrag",
                    "family": "lightrag",
                    "families": ["lightrag"],
                    "parameter_size": "1B",
                    "quantization_level": "Q4_0"
                }
            }
            for mode in QueryMode
        ]
    }
```

## ğŸ“Š æ€§èƒ½ç›‘æ§ä¸ä¼˜åŒ– (2025ç‰ˆ)

### å®æ—¶æ€§èƒ½ç›‘æ§
```python
import time
import psutil
from prometheus_client import Counter, Histogram, generate_latest

# PrometheusæŒ‡æ ‡
REQUEST_COUNT = Counter('lightrag_requests_total', 'Total requests', ['method', 'endpoint', 'status'])
REQUEST_DURATION = Histogram('lightrag_request_duration_seconds', 'Request duration', ['method', 'endpoint'])

@app.middleware("http")
async def performance_monitor(request: Request, call_next):
    """2025å¹´æ€§èƒ½ç›‘æ§ä¸­é—´ä»¶"""
    start_time = time.time()
    method = request.method
    endpoint = str(request.url.path)
    
    # ç³»ç»Ÿèµ„æºç›‘æ§
    cpu_before = psutil.cpu_percent()
    memory_before = psutil.virtual_memory().percent
    
    response = await call_next(request)
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    duration = time.time() - start_time
    cpu_after = psutil.cpu_percent()
    memory_after = psutil.virtual_memory().percent
    
    # è®°å½•PrometheusæŒ‡æ ‡
    REQUEST_COUNT.labels(method=method, endpoint=endpoint, status=response.status_code).inc()
    REQUEST_DURATION.labels(method=method, endpoint=endpoint).observe(duration)
    
    # æ€§èƒ½æ—¥å¿—
    if duration > 1.0:  # æ…¢æŸ¥è¯¢å‘Šè­¦
        logger.warning(
            f"æ…¢æŸ¥è¯¢æ£€æµ‹ - {method} {endpoint}: "
            f"è€—æ—¶{duration:.2f}s, "
            f"CPU: {cpu_before}%->{cpu_after}%, "
            f"å†…å­˜: {memory_before}%->{memory_after}%"
        )
    
    # æ·»åŠ æ€§èƒ½å¤´éƒ¨
    response.headers["X-Process-Time"] = str(duration)
    response.headers["X-CPU-Usage"] = f"{cpu_after}%"
    response.headers["X-Memory-Usage"] = f"{memory_after}%"
    
    return response

@app.get("/metrics")
async def prometheus_metrics():
    """PrometheusæŒ‡æ ‡ç«¯ç‚¹"""
    return Response(generate_latest(), media_type="text/plain")
```

## ğŸš€ ç”Ÿäº§éƒ¨ç½²é…ç½® (2025ç‰ˆ)

### Gunicorn + Uvicornæœ€ä½³é…ç½®
```python
# gunicorn_config.py - 2025ä¼˜åŒ–ç‰ˆ
import multiprocessing
import os

# åŸºç¡€é…ç½®
bind = "0.0.0.0:9621"
workers = int(os.getenv("WORKERS", multiprocessing.cpu_count() * 2 + 1))
worker_class = "uvicorn.workers.UvicornWorker"
worker_connections = 1000

# æ€§èƒ½ä¼˜åŒ– (2025æ–°å¢)
max_requests = 1000
max_requests_jitter = 100
preload_app = True
keepalive = 2
timeout = 240
graceful_timeout = 30

# æ—¥å¿—é…ç½®
accesslog = "-"
errorlog = "-"
loglevel = "info"
access_log_format = '%(h)s %(l)s %(u)s %(t)s "%(r)s" %(s)s %(b)s "%(f)s" "%(a)s" %(D)s'

# è¿›ç¨‹ç®¡ç†
proc_name = "lightrag-api"
pidfile = "/tmp/lightrag-api.pid"
user = os.getenv("USER", "lightrag")
group = os.getenv("GROUP", "lightrag")

# å®‰å…¨é…ç½®
limit_request_line = 8192
limit_request_fields = 100
limit_request_field_size = 8190

# éƒ¨ç½²å‘½ä»¤ç¤ºä¾‹
"""
gunicorn lightrag.api.lightrag_server:app \
    -c gunicorn_config.py \
    --reload  # ä»…å¼€å‘ç¯å¢ƒ
"""
```

## âš ï¸ æ³¨æ„äº‹é¡¹ (2025æ›´æ–°)

1. **å¼‚æ­¥ä¼˜å…ˆ**: æ‰€æœ‰I/Oæ“ä½œä½¿ç”¨async/awaitï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
2. **æ€§èƒ½ç›‘æ§**: ä½¿ç”¨py-spyè¿›è¡Œç”Ÿäº§ç¯å¢ƒæ€§èƒ½åˆ†æ
3. **è¶…æ—¶æ§åˆ¶**: æ‰€æœ‰å¤–éƒ¨è°ƒç”¨è®¾ç½®åˆç†è¶…æ—¶æ—¶é—´
4. **é”™è¯¯å¤„ç†**: å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œä¼˜é›…é™çº§
5. **èµ„æºç®¡ç†**: æ­£ç¡®ç®¡ç†æ•°æ®åº“è¿æ¥æ± å’ŒRedisè¿æ¥
6. **å®‰å…¨è€ƒè™‘**: è¾“å…¥éªŒè¯ã€é€Ÿç‡é™åˆ¶ã€APIå¯†é’¥ç®¡ç†
7. **æ—¥å¿—è®°å½•**: ç»“æ„åŒ–æ—¥å¿—ï¼Œä¾¿äºé—®é¢˜æ’æŸ¥
8. **ç¼“å­˜ç­–ç•¥**: åˆç†ä½¿ç”¨Redisç¼“å­˜ï¼Œæå‡å“åº”é€Ÿåº¦
9. **é‡æ’åºä¼˜åŒ–**: æ ¹æ®æŸ¥è¯¢å¤æ‚åº¦åŠ¨æ€å¯ç”¨é‡æ’åº
10. **å…¼å®¹æ€§ç»´æŠ¤**: ä¿æŒOllama APIçš„å®Œå…¨å…¼å®¹æ€§
