---
description: LightRAG LLM é›†æˆå¼€å‘è§„åˆ™
alwaysApply: false
---
# LightRAG LLM é›†æˆå¼€å‘è§„åˆ™

## ğŸ¤– LLMæ¶æ„æ¦‚è§ˆ

LightRAGæ”¯æŒçµæ´»çš„å¤šæ¨¡å‹é…ç½®ç­–ç•¥ï¼Œå¯ä»¥ä¸ºä¸åŒçš„AIä»»åŠ¡é…ç½®ä¸åŒçš„æ¨¡å‹ï¼Œå®ç°æˆæœ¬ä¸æ€§èƒ½çš„æœ€ä½³å¹³è¡¡ã€‚æ ¹æ®ç”¨æˆ·é…ç½®ï¼Œå½“å‰ä½¿ç”¨ï¼š

### ä»»åŠ¡ç‰¹åŒ–é…ç½®
- **å®ä½“æŠ½å–**: SiliconFlow Qwen2.5-7B (å¿«é€Ÿ+ç»æµ)
- **å®ä½“æ€»ç»“**: SiliconFlow Qwen2.5-72B (ç†è§£èƒ½åŠ›å¼º)
- **å…³ç³»æ€»ç»“**: SiliconFlow Qwen2.5-72B (é€»è¾‘æ¨ç†å¼º)
- **æŸ¥è¯¢å“åº”**: OpenRouter Qwen3-235B (å¯¹è¯èƒ½åŠ›å¼º)
- **å…³é”®è¯æå–**: SiliconFlow Qwen2.5-7B (å¿«é€Ÿå¤„ç†)

### æˆæœ¬ç­–ç•¥çŸ©é˜µ
| ä»»åŠ¡ç±»å‹ | å¤æ‚åº¦ | æ¨èæ¨¡å‹ | æˆæœ¬ç­‰çº§ | æ€§èƒ½è¦æ±‚ |
|---------|--------|----------|----------|----------|
| **å®ä½“æå–** | ä½ | SiliconFlow 7B | ğŸ’° | é€Ÿåº¦ä¼˜å…ˆ |
| **å…³é”®è¯æå–** | ä½ | SiliconFlow 7B | ğŸ’° | é€Ÿåº¦ä¼˜å…ˆ |
| **å®ä½“æ‘˜è¦** | ä¸­ | SiliconFlow 72B | ğŸ’°ğŸ’° | è´¨é‡å¹³è¡¡ |
| **å…³ç³»æ‘˜è¦** | ä¸­ | SiliconFlow 72B | ğŸ’°ğŸ’° | è´¨é‡å¹³è¡¡ |
| **æŸ¥è¯¢å“åº”** | é«˜ | OpenAI GPT-4o | ğŸ’°ğŸ’°ğŸ’° | è´¨é‡ä¼˜å…ˆ |

## ğŸ“‚ LLMæ¨¡å—ç»“æ„

```
lightrag/llm/
â”œâ”€â”€ __init__.py                 # LLMå·¥å‚å’Œé€šç”¨æ¥å£
â”œâ”€â”€ openai.py                   # OpenAI/OpenAIå…¼å®¹æ¥å£
â”œâ”€â”€ siliconflow.py             # ç¡…åŸºæµåŠ¨ (æ€§ä»·æ¯”ä¹‹é€‰)
â”œâ”€â”€ openrouter.py              # OpenRouter (æ¨¡å‹èšåˆ)
â”œâ”€â”€ gemini.py                  # Google Gemini
â”œâ”€â”€ azure_openai.py            # Azure OpenAI
â”œâ”€â”€ ollama.py                  # Ollamaæœ¬åœ°æ¨¡å‹
â”œâ”€â”€ zhipu.py                   # æ™ºè°±AI (ä¸­æ–‡ä¼˜åŒ–)
â”œâ”€â”€ anthropic.py               # Anthropic Claude
â”œâ”€â”€ nvidia_openai.py           # NVIDIA API
â”œâ”€â”€ bedrock.py                 # AWS Bedrock
â”œâ”€â”€ hf.py                      # HuggingFace
â”œâ”€â”€ jina.py                    # Jina AI
â”œâ”€â”€ lmdeploy.py                # LMDeploy
â””â”€â”€ llama_index_impl.py        # LlamaIndexé›†æˆ
```

## ğŸ”§ LLMæ¥å£è§„èŒƒ

### 1. ç»Ÿä¸€å¼‚æ­¥æ¥å£
```python
async def llm_func(
    prompt: str,
    system_prompt: str = None,
    history_messages: List[dict] = None,
    keyword_extraction: bool = False,
    **kwargs
) -> str:
    """
    LLMè°ƒç”¨çš„ç»Ÿä¸€æ¥å£
    
    Args:
        prompt: ç”¨æˆ·æç¤ºè¯
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        history_messages: å†å²å¯¹è¯
        keyword_extraction: æ˜¯å¦ä¸ºå…³é”®è¯æå–ä»»åŠ¡
        **kwargs: å…¶ä»–å‚æ•° (temperature, max_tokensç­‰)
    
    Returns:
        str: LLMå“åº”ç»“æœ
    """
    pass
```

### 2. æµå¼å“åº”æ¥å£
```python
async def llm_stream_func(
    prompt: str,
    system_prompt: str = None,
    history_messages: List[dict] = None,
    **kwargs
) -> AsyncIterator[str]:
    """æµå¼LLMå“åº”"""
    async for chunk in stream_response:
        yield chunk
```

### 3. åµŒå…¥æ¨¡å‹æ¥å£
```python
async def embedding_func(texts: List[str]) -> np.ndarray:
    """
    æ–‡æœ¬åµŒå…¥æ¥å£
    
    Args:
        texts: å¾…åµŒå…¥çš„æ–‡æœ¬åˆ—è¡¨
    
    Returns:
        np.ndarray: åµŒå…¥å‘é‡çŸ©é˜µï¼Œå½¢çŠ¶ä¸º (len(texts), embedding_dim)
    """
    pass
```

## ğŸ¯ ä»»åŠ¡ç‰¹åŒ–é…ç½®

### 1. å¤šæ¨¡å‹ä»»åŠ¡åˆ†é…
```python
# æ ¹æ®ç”¨æˆ·é…ç½®çš„ä»»åŠ¡ç‰¹åŒ–LLM
LLM_CONFIG = {
    'entity_extraction': {
        'binding': 'siliconflow',
        'model': 'Qwen/Qwen2.5-7B-Instruct',
        'host': 'https://api.siliconflow.cn/v1',
        'api_key': os.getenv('ENTITY_EXTRACTION_LLM_BINDING_API_KEY'),
        'temperature': 0.1,
        'max_tokens': 32000,
        'priority': 6  # ä¸­ç­‰ä¼˜å…ˆçº§
    },
    'entity_summary': {
        'binding': 'siliconflow',
        'model': 'Qwen/Qwen2.5-72B-Instruct',
        'host': 'https://api.siliconflow.cn/v1',
        'api_key': os.getenv('ENTITY_SUMMARY_LLM_BINDING_API_KEY'),
        'temperature': 0.1,
        'priority': 7  # é«˜ä¼˜å…ˆçº§
    },
    'relation_summary': {
        'binding': 'siliconflow',
        'model': 'Qwen/Qwen2.5-72B-Instruct',
        'host': 'https://api.siliconflow.cn/v1',
        'api_key': os.getenv('RELATION_SUMMARY_LLM_BINDING_API_KEY'),
        'temperature': 0.1,
        'priority': 7  # é«˜ä¼˜å…ˆçº§
    },
    'query_response': {
        'binding': 'openrouter',
        'model': 'qwen/qwen3-235b-a22b-07-25:free',
        'host': 'https://openrouter.ai/api/v1',
        'api_key': os.getenv('QUERY_LLM_BINDING_API_KEY'),
        'temperature': 0.1,
        'priority': 8  # æœ€é«˜ä¼˜å…ˆçº§
    },
    'keyword_extraction': {
        'binding': 'siliconflow',
        'model': 'Qwen/Qwen2.5-7B-Instruct',
        'host': 'https://api.siliconflow.cn/v1',
        'api_key': os.getenv('KEYWORD_EXTRACTION_LLM_BINDING_API_KEY'),
        'temperature': 0.1,
        'priority': 5  # è¾ƒä½ä¼˜å…ˆçº§
    }
}
```

### 2. åŠ¨æ€æ¨¡å‹é€‰æ‹©
```python
def get_llm_func(task_type: str) -> Callable:
    """æ ¹æ®ä»»åŠ¡ç±»å‹è¿”å›å¯¹åº”çš„LLMå‡½æ•°"""
    config = LLM_CONFIG.get(task_type, LLM_CONFIG.get('default'))
    
    if not config:
        raise ValueError(f"æœªæ‰¾åˆ°ä»»åŠ¡ç±»å‹ {task_type} çš„é…ç½®")
    
    # æ ¹æ®ç»‘å®šç±»å‹é€‰æ‹©å¯¹åº”çš„å®ç°
    binding_map = {
        'siliconflow': siliconflow_llm_func,
        'openrouter': openrouter_llm_func,
        'openai': openai_llm_func,
        'gemini': gemini_llm_func,
        'zhipu': zhipu_llm_func,
        'ollama': ollama_llm_func,
        'anthropic': anthropic_llm_func
    }
    
    llm_func = binding_map.get(config['binding'])
    if not llm_func:
        raise ValueError(f"ä¸æ”¯æŒçš„LLMç»‘å®šç±»å‹: {config['binding']}")
    
    return partial(llm_func, config=config)
```

### 3. åŒå±‚å…³é”®è¯æå–
```python
async def get_keywords_from_query(
    query: str, 
    llm_func: Callable,
    extract_type: str = "both"  # "hl", "ll", "both"
) -> Tuple[List[str], List[str]]:
    """
    ä»æŸ¥è¯¢ä¸­æå–åŒå±‚å…³é”®è¯
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        llm_func: LLMå‡½æ•°
        extract_type: æå–ç±»å‹ (hl=é«˜çº§, ll=ä½çº§, both=åŒå±‚)
    
    Returns:
        Tuple[List[str], List[str]]: (high_level_keywords, low_level_keywords)
    """
    
    hl_keywords, ll_keywords = [], []
    
    if extract_type in ["hl", "both"]:
        # é«˜çº§å…³é”®è¯æå– (æ¦‚å¿µæ€§ã€æŠ½è±¡æ€§)
        hl_prompt = f"""
        ä»ä»¥ä¸‹æŸ¥è¯¢ä¸­æå–é«˜çº§å…³é”®è¯ï¼Œå…³æ³¨æ¦‚å¿µæ€§ã€æŠ½è±¡æ€§çš„å†…å®¹ï¼š
        æŸ¥è¯¢: {query}
        
        è¯·æå–å‡ºå…³é”®çš„æ¦‚å¿µã€ä¸»é¢˜ã€æŠ½è±¡æ¦‚å¿µç­‰é«˜çº§å…³é”®è¯ã€‚
        è¿”å›æ ¼å¼: keyword1, keyword2, keyword3
        """
        hl_result = await llm_func(hl_prompt, keyword_extraction=True)
        hl_keywords = [kw.strip() for kw in hl_result.split(',') if kw.strip()]
    
    if extract_type in ["ll", "both"]:
        # ä½çº§å…³é”®è¯æå– (å…·ä½“å®ä½“ã€äº‹å®æ€§)
        ll_prompt = f"""
        ä»ä»¥ä¸‹æŸ¥è¯¢ä¸­æå–ä½çº§å…³é”®è¯ï¼Œå…³æ³¨å…·ä½“çš„å®ä½“ã€äººåã€åœ°åã€äº‹å®ç­‰ï¼š
        æŸ¥è¯¢: {query}
        
        è¯·æå–å‡ºå…·ä½“çš„å®ä½“åç§°ã€ä¸“æœ‰åè¯ã€äº‹å®æ€§å…³é”®è¯ç­‰ã€‚
        è¿”å›æ ¼å¼: keyword1, keyword2, keyword3
        """
        ll_result = await llm_func(ll_prompt, keyword_extraction=True)
        ll_keywords = [kw.strip() for kw in ll_result.split(',') if kw.strip()]
    
    return hl_keywords, ll_keywords
```

## ğŸ”§ å®ç°å¼€å‘è§„èŒƒ

### 1. SiliconFlowå®ç° (æ€§ä»·æ¯”ä¹‹é€‰)
```python
async def siliconflow_llm_func(
    prompt: str,
    system_prompt: str = None,
    history_messages: List[dict] = None,
    config: dict = None,
    **kwargs
) -> str:
    """SiliconFlow LLMå®ç°"""
    
    headers = {
        "Authorization": f"Bearer {config['api_key']}",
        "Content-Type": "application/json"
    }
    
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    
    # æ·»åŠ å†å²æ¶ˆæ¯
    if history_messages:
        messages.extend(history_messages)
    
    messages.append({"role": "user", "content": prompt})
    
    data = {
        "model": config['model'],
        "messages": messages,
        "temperature": config.get('temperature', 0.1),
        "max_tokens": config.get('max_tokens', 32000),
        "stream": False
    }
    
    # æ·»åŠ æµå¼å¤„ç†æ”¯æŒ
    if kwargs.get('stream', False):
        data['stream'] = True
        return await _siliconflow_stream_request(config['host'], data, headers)
    
    async with httpx.AsyncClient(timeout=240) as client:
        response = await client.post(
            f"{config['host']}/chat/completions",
            json=data,
            headers=headers
        )
        
        if response.status_code != 200:
            raise Exception(f"SiliconFlow APIé”™è¯¯: {response.status_code} - {response.text}")
        
        result = response.json()
        return result['choices'][0]['message']['content']
```

### 2. OpenRouterå®ç° (æ¨¡å‹èšåˆ)
```python
async def openrouter_llm_func(
    prompt: str,
    system_prompt: str = None,
    config: dict = None,
    **kwargs
) -> str:
    """OpenRouter LLMå®ç°"""
    
    headers = {
        "Authorization": f"Bearer {config['api_key']}",
        "HTTP-Referer": "https://lightrag.dev",  # OpenRouterè¦æ±‚
        "X-Title": "LightRAG",
        "Content-Type": "application/json"
    }
    
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    
    messages.append({"role": "user", "content": prompt})
    
    data = {
        "model": config['model'],
        "messages": messages,
        "temperature": config.get('temperature', 0.1),
        "max_tokens": config.get('max_tokens', 32000)
    }
    
    async with httpx.AsyncClient(timeout=240) as client:
        response = await client.post(
            f"{config['host']}/chat/completions",
            json=data,
            headers=headers
        )
        
        if response.status_code != 200:
            raise Exception(f"OpenRouter APIé”™è¯¯: {response.status_code} - {response.text}")
        
        result = response.json()
        return result['choices'][0]['message']['content']
```

### 3. åµŒå…¥æ¨¡å‹å®ç° (BAAI/bge-m3)
```python
async def siliconflow_embedding_func(texts: List[str]) -> np.ndarray:
    """SiliconFlowåµŒå…¥æ¨¡å‹å®ç°"""
    api_key = os.getenv('EMBEDDING_BINDING_API_KEY')
    model = os.getenv('EMBEDDING_MODEL', 'BAAI/bge-m3')
    host = os.getenv('EMBEDDING_BINDING_HOST', 'https://api.siliconflow.cn/v1')
    embedding_dim = int(os.getenv('EMBEDDING_DIM', 1024))
    
    if not api_key:
        raise ValueError("ç¼ºå°‘åµŒå…¥æ¨¡å‹APIå¯†é’¥")
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    # æ‰¹é‡å¤„ç†ï¼Œé¿å…å•æ¬¡è¯·æ±‚è¿‡å¤§
    batch_size = int(os.getenv('EMBEDDING_BATCH_NUM', 50))
    all_embeddings = []
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        
        data = {
            "model": model,
            "input": batch,
            "encoding_format": "float"
        }
        
        async with httpx.AsyncClient(timeout=240) as client:
            response = await client.post(
                f"{host}/embeddings",
                json=data,
                headers=headers
            )
            
            if response.status_code != 200:
                raise Exception(f"åµŒå…¥æ¨¡å‹APIé”™è¯¯: {response.status_code} - {response.text}")
            
            result = response.json()
            batch_embeddings = [item['embedding'] for item in result['data']]
            all_embeddings.extend(batch_embeddings)
    
    embeddings_array = np.array(all_embeddings)
    
    # éªŒè¯ç»´åº¦
    if embeddings_array.shape[1] != embedding_dim:
        logger.warning(f"åµŒå…¥ç»´åº¦ä¸åŒ¹é…: æœŸæœ›{embedding_dim}, å®é™…{embeddings_array.shape[1]}")
    
    return embeddings_array
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. æ™ºèƒ½ç¼“å­˜æœºåˆ¶
```python
import hashlib
from functools import lru_cache
from typing import Optional

class LLMCache:
    def __init__(self, redis_client=None):
        self.redis = redis_client
        self.local_cache = {}
        self.cache_enabled = os.getenv('ENABLE_LLM_CACHE', 'true').lower() == 'true'
        self.extract_cache_enabled = os.getenv('ENABLE_LLM_CACHE_FOR_EXTRACT', 'true').lower() == 'true'
    
    def get_cache_key(self, prompt: str, config: dict) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        cache_data = {
            'prompt': prompt,
            'model': config.get('model'),
            'temperature': config.get('temperature'),
            'max_tokens': config.get('max_tokens')
        }
        cache_str = json.dumps(cache_data, sort_keys=True)
        return f"llm_cache:{hashlib.md5(cache_str.encode()).hexdigest()}"
    
    async def get(self, cache_key: str) -> Optional[str]:
        """è·å–ç¼“å­˜"""
        if not self.cache_enabled:
            return None
        
        # ä¼˜å…ˆä»Redisè·å–
        if self.redis:
            try:
                cached = await self.redis.get(cache_key)
                if cached:
                    return cached
            except Exception as e:
                logger.warning(f"Redisç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        # é™çº§åˆ°æœ¬åœ°ç¼“å­˜
        return self.local_cache.get(cache_key)
    
    async def set(self, cache_key: str, value: str, expire: int = 86400) -> None:
        """è®¾ç½®ç¼“å­˜"""
        if not self.cache_enabled:
            return
        
        # å­˜å‚¨åˆ°Redis
        if self.redis:
            try:
                await self.redis.set(cache_key, value, ex=expire)
            except Exception as e:
                logger.warning(f"Redisç¼“å­˜å†™å…¥å¤±è´¥: {e}")
        
        # å­˜å‚¨åˆ°æœ¬åœ°ç¼“å­˜
        self.local_cache[cache_key] = value
        
        # é™åˆ¶æœ¬åœ°ç¼“å­˜å¤§å°
        if len(self.local_cache) > 1000:
            # åˆ é™¤æœ€è€çš„ç¼“å­˜é¡¹
            oldest_key = next(iter(self.local_cache))
            del self.local_cache[oldest_key]

# å…¨å±€ç¼“å­˜å®ä¾‹
llm_cache = LLMCache()

async def cached_llm_call(prompt: str, config: dict, **kwargs) -> str:
    """å¸¦ç¼“å­˜çš„LLMè°ƒç”¨"""
    cache_key = llm_cache.get_cache_key(prompt, config)
    
    # å°è¯•ä»ç¼“å­˜è·å–
    cached_result = await llm_cache.get(cache_key)
    if cached_result:
        logger.debug(f"LLMç¼“å­˜å‘½ä¸­: {cache_key[:16]}...")
        return cached_result
    
    # ç¼“å­˜æœªå‘½ä¸­ï¼Œè°ƒç”¨LLM
    result = await actual_llm_call(prompt, config, **kwargs)
    
    # ä¿å­˜åˆ°ç¼“å­˜
    await llm_cache.set(cache_key, result)
    
    return result
```

### 2. å¹¶å‘æ§åˆ¶
```python
import asyncio
from asyncio import Semaphore
from collections import defaultdict

# æ ¹æ®ç”¨æˆ·é…ç½®æ§åˆ¶å¹¶å‘
MAX_ASYNC = int(os.getenv('MAX_ASYNC', 16))
EMBEDDING_MAX_ASYNC = int(os.getenv('EMBEDDING_FUNC_MAX_ASYNC', 32))

# ä¸åŒä»»åŠ¡ç±»å‹çš„ä¿¡å·é‡
task_semaphores = {
    'entity_extraction': Semaphore(8),    # å®ä½“æå–ï¼šé«˜å¹¶å‘
    'keyword_extraction': Semaphore(8),   # å…³é”®è¯æå–ï¼šé«˜å¹¶å‘
    'entity_summary': Semaphore(4),       # å®ä½“æ‘˜è¦ï¼šä¸­ç­‰å¹¶å‘
    'relation_summary': Semaphore(4),     # å…³ç³»æ‘˜è¦ï¼šä¸­ç­‰å¹¶å‘
    'query_response': Semaphore(2),       # æŸ¥è¯¢å“åº”ï¼šä½å¹¶å‘
    'embedding': Semaphore(EMBEDDING_MAX_ASYNC)
}

async def controlled_llm_call(
    prompt: str, 
    task_type: str = 'default',
    **kwargs
) -> str:
    """å¸¦å¹¶å‘æ§åˆ¶çš„LLMè°ƒç”¨"""
    semaphore = task_semaphores.get(task_type, task_semaphores['query_response'])
    
    async with semaphore:
        return await cached_llm_call(prompt, **kwargs)
```

### 3. ä¼˜å…ˆçº§é˜Ÿåˆ—
```python
import heapq
from dataclasses import dataclass
from typing import Any, Callable

@dataclass
class LLMTask:
    priority: int  # æ•°å­—è¶Šå¤§ä¼˜å…ˆçº§è¶Šé«˜
    task_id: str
    prompt: str
    config: dict
    callback: Callable
    timestamp: float = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = time.time()
    
    def __lt__(self, other):
        # ä¼˜å…ˆçº§é«˜çš„æ’åœ¨å‰é¢ï¼Œå¦‚æœä¼˜å…ˆçº§ç›¸åŒåˆ™æŒ‰æ—¶é—´æ’åº
        if self.priority != other.priority:
            return self.priority > other.priority
        return self.timestamp < other.timestamp

class LLMTaskScheduler:
    def __init__(self):
        self.task_queue = []
        self.running_tasks = set()
        self.max_concurrent = MAX_ASYNC
        
    async def submit_task(self, task: LLMTask):
        """æäº¤ä»»åŠ¡åˆ°ä¼˜å…ˆçº§é˜Ÿåˆ—"""
        heapq.heappush(self.task_queue, task)
        await self._process_queue()
    
    async def _process_queue(self):
        """å¤„ç†ä»»åŠ¡é˜Ÿåˆ—"""
        while (len(self.running_tasks) < self.max_concurrent 
               and self.task_queue):
            
            task = heapq.heappop(self.task_queue)
            self.running_tasks.add(task.task_id)
            
            # å¼‚æ­¥æ‰§è¡Œä»»åŠ¡
            asyncio.create_task(self._execute_task(task))
    
    async def _execute_task(self, task: LLMTask):
        """æ‰§è¡ŒLLMä»»åŠ¡"""
        try:
            result = await cached_llm_call(task.prompt, task.config)
            await task.callback(result)
        except Exception as e:
            logger.error(f"LLMä»»åŠ¡æ‰§è¡Œå¤±è´¥ {task.task_id}: {e}")
        finally:
            self.running_tasks.discard(task.task_id)
            # ç»§ç»­å¤„ç†é˜Ÿåˆ—
            await self._process_queue()

# å…¨å±€ä»»åŠ¡è°ƒåº¦å™¨
llm_scheduler = LLMTaskScheduler()
```

## ğŸ” é”™è¯¯å¤„ç†å’Œé‡è¯•

### 1. æ™ºèƒ½é‡è¯•æœºåˆ¶
```python
import backoff
from typing import Type, Tuple

class LLMError(Exception):
    """LLMè°ƒç”¨é”™è¯¯åŸºç±»"""
    pass

class LLMRateLimitError(LLMError):
    """é€Ÿç‡é™åˆ¶é”™è¯¯"""
    pass

class LLMServerError(LLMError):
    """æœåŠ¡å™¨é”™è¯¯"""
    pass

@backoff.on_exception(
    backoff.expo,
    (httpx.RequestError, httpx.HTTPStatusError, LLMServerError),
    max_tries=3,
    max_time=300,
    giveup=lambda e: isinstance(e, LLMRateLimitError)
)
async def robust_llm_call(prompt: str, config: dict, **kwargs) -> str:
    """å¸¦é‡è¯•æœºåˆ¶çš„LLMè°ƒç”¨"""
    try:
        return await llm_func(prompt, config, **kwargs)
    except httpx.HTTPStatusError as e:
        if e.response.status_code == 429:  # Rate limit
            raise LLMRateLimitError(f"APIé€Ÿç‡é™åˆ¶: {e.response.text}")
        elif e.response.status_code >= 500:  # Server error
            raise LLMServerError(f"æœåŠ¡å™¨é”™è¯¯: {e.response.text}")
        else:
            logger.error(f"HTTPé”™è¯¯ {e.response.status_code}: {e.response.text}")
            raise LLMError(f"APIè°ƒç”¨å¤±è´¥: {e.response.text}")
    except Exception as e:
        logger.error(f"LLMè°ƒç”¨æ„å¤–é”™è¯¯: {e}")
        raise LLMError(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")
```

### 2. é™çº§ç­–ç•¥
```python
async def fallback_llm_call(
    prompt: str, 
    primary_config: dict,
    fallback_config: dict = None,
    **kwargs
) -> str:
    """LLMè°ƒç”¨é™çº§ç­–ç•¥"""
    
    # å°è¯•ä¸»è¦æ¨¡å‹
    try:
        return await robust_llm_call(prompt, primary_config, **kwargs)
    except LLMRateLimitError:
        # é€Ÿç‡é™åˆ¶æ—¶ç­‰å¾…é‡è¯•
        await asyncio.sleep(60)
        raise
    except Exception as e:
        logger.warning(f"ä¸»è¦LLMå¤±è´¥: {e}")
        
        # å¦‚æœæœ‰é™çº§é…ç½®ï¼Œå°è¯•é™çº§æ¨¡å‹
        if fallback_config:
            try:
                logger.info(f"åˆ‡æ¢åˆ°é™çº§æ¨¡å‹: {fallback_config['model']}")
                return await robust_llm_call(prompt, fallback_config, **kwargs)
            except Exception as fallback_error:
                logger.error(f"é™çº§æ¨¡å‹ä¹Ÿå¤±è´¥: {fallback_error}")
        
        # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºåŸå§‹é”™è¯¯
        raise e
```

## ğŸ”§ é…ç½®ç®¡ç†

### ç¯å¢ƒå˜é‡é…ç½®æ˜ å°„
```python
def load_llm_config() -> dict:
    """ä»ç¯å¢ƒå˜é‡åŠ è½½LLMé…ç½®"""
    config = {}
    
    # å…¨å±€é»˜è®¤é…ç½®
    config['default'] = {
        'binding': os.getenv('LLM_BINDING', 'siliconflow'),
        'model': os.getenv('LLM_MODEL', 'Qwen/Qwen2.5-14B-Instruct'),
        'host': os.getenv('LLM_BINDING_HOST', 'https://api.siliconflow.cn/v1'),
        'api_key': os.getenv('LLM_BINDING_API_KEY'),
        'temperature': float(os.getenv('TEMPERATURE', 0.1)),
        'timeout': int(os.getenv('TIMEOUT', 240)),
        'max_tokens': int(os.getenv('MAX_TOKENS', 32000))
    }
    
    # ä»»åŠ¡ç‰¹åŒ–é…ç½®
    for task in ['entity_extraction', 'entity_summary', 'relation_summary', 
                 'query_response', 'keyword_extraction']:
        task_config = {}
        prefix = task.upper()
        
        for key in ['BINDING', 'MODEL', 'BINDING_HOST', 'BINDING_API_KEY']:
            env_key = f"{prefix}_LLM_{key}"
            value = os.getenv(env_key)
            if value:
                config_key = key.lower().replace('binding_', '')
                task_config[config_key] = value
        
        if task_config:
            config[task] = {**config['default'], **task_config}
    
    # åµŒå…¥æ¨¡å‹é…ç½®
    config['embedding'] = {
        'binding': os.getenv('EMBEDDING_BINDING', 'siliconflow'),
        'model': os.getenv('EMBEDDING_MODEL', 'BAAI/bge-m3'),
        'host': os.getenv('EMBEDDING_BINDING_HOST', 'https://api.siliconflow.cn/v1'),
        'api_key': os.getenv('EMBEDDING_BINDING_API_KEY'),
        'dim': int(os.getenv('EMBEDDING_DIM', 1024)),
        'batch_size': int(os.getenv('EMBEDDING_BATCH_NUM', 50))
    }
    
    return config
```

## ğŸ§ª æµ‹è¯•è§„èŒƒ

### 1. LLMåŠŸèƒ½æµ‹è¯•
```python
@pytest.mark.asyncio
async def test_llm_functionality():
    """æµ‹è¯•LLMåŸºæœ¬åŠŸèƒ½"""
    config = {
        'binding': 'siliconflow',
        'model': 'Qwen/Qwen2.5-7B-Instruct',
        'api_key': 'test_key'
    }
    
    prompt = "æµ‹è¯•LLMåŠŸèƒ½"
    response = await siliconflow_llm_func(prompt, config=config)
    
    assert isinstance(response, str)
    assert len(response) > 0

@pytest.mark.asyncio
async def test_task_specialization():
    """æµ‹è¯•ä»»åŠ¡ç‰¹åŒ–é…ç½®"""
    tasks = ['entity_extraction', 'entity_summary', 'query_response']
    
    for task in tasks:
        llm_func = get_llm_func(task)
        assert llm_func is not None
        
        # æµ‹è¯•ä¸åŒä»»åŠ¡çš„å“åº”
        response = await llm_func("æµ‹è¯•ä»»åŠ¡ç‰¹åŒ–")
        assert isinstance(response, str)
```

### 2. åµŒå…¥åŠŸèƒ½æµ‹è¯•
```python
@pytest.mark.asyncio
async def test_embedding_functionality():
    """æµ‹è¯•åµŒå…¥æ¨¡å‹åŠŸèƒ½"""
    texts = ["æµ‹è¯•æ–‡æœ¬1", "æµ‹è¯•æ–‡æœ¬2", "æµ‹è¯•æ–‡æœ¬3"]
    embeddings = await siliconflow_embedding_func(texts)
    
    expected_dim = int(os.getenv('EMBEDDING_DIM', 1024))
    
    assert embeddings.shape[0] == len(texts)
    assert embeddings.shape[1] == expected_dim
    assert np.all(np.isfinite(embeddings))  # ç¡®ä¿æ²¡æœ‰NaNæˆ–Infå€¼
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **APIå¯†é’¥å®‰å…¨**: ä¸è¦åœ¨ä»£ç ä¸­ç¡¬ç¼–ç APIå¯†é’¥ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡
2. **ä»»åŠ¡ç‰¹åŒ–**: åˆç†é…ç½®ä¸åŒä»»åŠ¡çš„æ¨¡å‹ä»¥ä¼˜åŒ–æˆæœ¬å’Œæ€§èƒ½
3. **é€Ÿç‡é™åˆ¶**: éµå®ˆå„æä¾›å•†çš„APIè°ƒç”¨é™åˆ¶ï¼Œå®ç°æ™ºèƒ½é‡è¯•
4. **æˆæœ¬æ§åˆ¶**: ç›‘æ§APIè°ƒç”¨æˆæœ¬ï¼Œåˆç†é€‰æ‹©æ¨¡å‹
5. **é”™è¯¯å¤„ç†**: å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œé™çº§æœºåˆ¶
6. **æ¨¡å‹ä¸€è‡´æ€§**: åµŒå…¥æ¨¡å‹åœ¨æ•´ä¸ªç³»ç»Ÿä¸­ä¿æŒä¸€è‡´
7. **ç¼“å­˜ç­–ç•¥**: åˆç†ä½¿ç”¨ç¼“å­˜å‡å°‘é‡å¤è°ƒç”¨
8. **å¹¶å‘æ§åˆ¶**: æ ¹æ®ä»»åŠ¡ç±»å‹è®¾ç½®åˆé€‚çš„å¹¶å‘é™åˆ¶
9. **ä¼˜å…ˆçº§ç®¡ç†**: é‡è¦ä»»åŠ¡è®¾ç½®æ›´é«˜ä¼˜å…ˆçº§
10. **è¶…æ—¶è®¾ç½®**: åˆç†è®¾ç½®è¶…æ—¶æ—¶é—´é¿å…é•¿æ—¶é—´ç­‰å¾…
# LightRAG LLM é›†æˆå¼€å‘è§„åˆ™

## ğŸ¤– LLMæ¶æ„æ¦‚è§ˆ

LightRAGæ”¯æŒçµæ´»çš„å¤šæ¨¡å‹é…ç½®ç­–ç•¥ï¼Œå¯ä»¥ä¸ºä¸åŒçš„AIä»»åŠ¡é…ç½®ä¸åŒçš„æ¨¡å‹ï¼Œå®ç°æˆæœ¬ä¸æ€§èƒ½çš„æœ€ä½³å¹³è¡¡ã€‚æ ¹æ®ç”¨æˆ·é…ç½®ï¼Œå½“å‰ä½¿ç”¨ï¼š

### ä»»åŠ¡ç‰¹åŒ–é…ç½®
- **å®ä½“æŠ½å–**: SiliconFlow Qwen2.5-7B (å¿«é€Ÿ+ç»æµ)
- **å®ä½“æ€»ç»“**: SiliconFlow Qwen2.5-72B (ç†è§£èƒ½åŠ›å¼º)
- **å…³ç³»æ€»ç»“**: SiliconFlow Qwen2.5-72B (é€»è¾‘æ¨ç†å¼º)
- **æŸ¥è¯¢å“åº”**: OpenRouter Qwen3-235B (å¯¹è¯èƒ½åŠ›å¼º)
- **å…³é”®è¯æå–**: SiliconFlow Qwen2.5-7B (å¿«é€Ÿå¤„ç†)

### æˆæœ¬ç­–ç•¥çŸ©é˜µ
| ä»»åŠ¡ç±»å‹ | å¤æ‚åº¦ | æ¨èæ¨¡å‹ | æˆæœ¬ç­‰çº§ | æ€§èƒ½è¦æ±‚ |
|---------|--------|----------|----------|----------|
| **å®ä½“æå–** | ä½ | SiliconFlow 7B | ğŸ’° | é€Ÿåº¦ä¼˜å…ˆ |
| **å…³é”®è¯æå–** | ä½ | SiliconFlow 7B | ğŸ’° | é€Ÿåº¦ä¼˜å…ˆ |
| **å®ä½“æ‘˜è¦** | ä¸­ | SiliconFlow 72B | ğŸ’°ğŸ’° | è´¨é‡å¹³è¡¡ |
| **å…³ç³»æ‘˜è¦** | ä¸­ | SiliconFlow 72B | ğŸ’°ğŸ’° | è´¨é‡å¹³è¡¡ |
| **æŸ¥è¯¢å“åº”** | é«˜ | OpenAI GPT-4o | ğŸ’°ğŸ’°ğŸ’° | è´¨é‡ä¼˜å…ˆ |

## ğŸ“‚ LLMæ¨¡å—ç»“æ„

```
lightrag/llm/
â”œâ”€â”€ __init__.py                 # LLMå·¥å‚å’Œé€šç”¨æ¥å£
â”œâ”€â”€ openai.py                   # OpenAI/OpenAIå…¼å®¹æ¥å£
â”œâ”€â”€ siliconflow.py             # ç¡…åŸºæµåŠ¨ (æ€§ä»·æ¯”ä¹‹é€‰)
â”œâ”€â”€ openrouter.py              # OpenRouter (æ¨¡å‹èšåˆ)
â”œâ”€â”€ gemini.py                  # Google Gemini
â”œâ”€â”€ azure_openai.py            # Azure OpenAI
â”œâ”€â”€ ollama.py                  # Ollamaæœ¬åœ°æ¨¡å‹
â”œâ”€â”€ zhipu.py                   # æ™ºè°±AI (ä¸­æ–‡ä¼˜åŒ–)
â”œâ”€â”€ anthropic.py               # Anthropic Claude
â”œâ”€â”€ nvidia_openai.py           # NVIDIA API
â”œâ”€â”€ bedrock.py                 # AWS Bedrock
â”œâ”€â”€ hf.py                      # HuggingFace
â”œâ”€â”€ jina.py                    # Jina AI
â”œâ”€â”€ lmdeploy.py                # LMDeploy
â””â”€â”€ llama_index_impl.py        # LlamaIndexé›†æˆ
```

## ğŸ”§ LLMæ¥å£è§„èŒƒ

### 1. ç»Ÿä¸€å¼‚æ­¥æ¥å£
```python
async def llm_func(
    prompt: str,
    system_prompt: str = None,
    history_messages: List[dict] = None,
    keyword_extraction: bool = False,
    **kwargs
) -> str:
    """
    LLMè°ƒç”¨çš„ç»Ÿä¸€æ¥å£
    
    Args:
        prompt: ç”¨æˆ·æç¤ºè¯
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        history_messages: å†å²å¯¹è¯
        keyword_extraction: æ˜¯å¦ä¸ºå…³é”®è¯æå–ä»»åŠ¡
        **kwargs: å…¶ä»–å‚æ•° (temperature, max_tokensç­‰)
    
    Returns:
        str: LLMå“åº”ç»“æœ
    """
    pass
```

### 2. æµå¼å“åº”æ¥å£
```python
async def llm_stream_func(
    prompt: str,
    system_prompt: str = None,
    history_messages: List[dict] = None,
    **kwargs
) -> AsyncIterator[str]:
    """æµå¼LLMå“åº”"""
    async for chunk in stream_response:
        yield chunk
```

### 3. åµŒå…¥æ¨¡å‹æ¥å£
```python
async def embedding_func(texts: List[str]) -> np.ndarray:
    """
    æ–‡æœ¬åµŒå…¥æ¥å£
    
    Args:
        texts: å¾…åµŒå…¥çš„æ–‡æœ¬åˆ—è¡¨
    
    Returns:
        np.ndarray: åµŒå…¥å‘é‡çŸ©é˜µï¼Œå½¢çŠ¶ä¸º (len(texts), embedding_dim)
    """
    pass
```

## ğŸ¯ ä»»åŠ¡ç‰¹åŒ–é…ç½®

### 1. å¤šæ¨¡å‹ä»»åŠ¡åˆ†é…
```python
# æ ¹æ®ç”¨æˆ·é…ç½®çš„ä»»åŠ¡ç‰¹åŒ–LLM
LLM_CONFIG = {
    'entity_extraction': {
        'binding': 'siliconflow',
        'model': 'Qwen/Qwen2.5-7B-Instruct',
        'host': 'https://api.siliconflow.cn/v1',
        'api_key': os.getenv('ENTITY_EXTRACTION_LLM_BINDING_API_KEY'),
        'temperature': 0.1,
        'max_tokens': 32000,
        'priority': 6  # ä¸­ç­‰ä¼˜å…ˆçº§
    },
    'entity_summary': {
        'binding': 'siliconflow',
        'model': 'Qwen/Qwen2.5-72B-Instruct',
        'host': 'https://api.siliconflow.cn/v1',
        'api_key': os.getenv('ENTITY_SUMMARY_LLM_BINDING_API_KEY'),
        'temperature': 0.1,
        'priority': 7  # é«˜ä¼˜å…ˆçº§
    },
    'relation_summary': {
        'binding': 'siliconflow',
        'model': 'Qwen/Qwen2.5-72B-Instruct',
        'host': 'https://api.siliconflow.cn/v1',
        'api_key': os.getenv('RELATION_SUMMARY_LLM_BINDING_API_KEY'),
        'temperature': 0.1,
        'priority': 7  # é«˜ä¼˜å…ˆçº§
    },
    'query_response': {
        'binding': 'openrouter',
        'model': 'qwen/qwen3-235b-a22b-07-25:free',
        'host': 'https://openrouter.ai/api/v1',
        'api_key': os.getenv('QUERY_LLM_BINDING_API_KEY'),
        'temperature': 0.1,
        'priority': 8  # æœ€é«˜ä¼˜å…ˆçº§
    },
    'keyword_extraction': {
        'binding': 'siliconflow',
        'model': 'Qwen/Qwen2.5-7B-Instruct',
        'host': 'https://api.siliconflow.cn/v1',
        'api_key': os.getenv('KEYWORD_EXTRACTION_LLM_BINDING_API_KEY'),
        'temperature': 0.1,
        'priority': 5  # è¾ƒä½ä¼˜å…ˆçº§
    }
}
```

### 2. åŠ¨æ€æ¨¡å‹é€‰æ‹©
```python
def get_llm_func(task_type: str) -> Callable:
    """æ ¹æ®ä»»åŠ¡ç±»å‹è¿”å›å¯¹åº”çš„LLMå‡½æ•°"""
    config = LLM_CONFIG.get(task_type, LLM_CONFIG.get('default'))
    
    if not config:
        raise ValueError(f"æœªæ‰¾åˆ°ä»»åŠ¡ç±»å‹ {task_type} çš„é…ç½®")
    
    # æ ¹æ®ç»‘å®šç±»å‹é€‰æ‹©å¯¹åº”çš„å®ç°
    binding_map = {
        'siliconflow': siliconflow_llm_func,
        'openrouter': openrouter_llm_func,
        'openai': openai_llm_func,
        'gemini': gemini_llm_func,
        'zhipu': zhipu_llm_func,
        'ollama': ollama_llm_func,
        'anthropic': anthropic_llm_func
    }
    
    llm_func = binding_map.get(config['binding'])
    if not llm_func:
        raise ValueError(f"ä¸æ”¯æŒçš„LLMç»‘å®šç±»å‹: {config['binding']}")
    
    return partial(llm_func, config=config)
```

### 3. åŒå±‚å…³é”®è¯æå–
```python
async def get_keywords_from_query(
    query: str, 
    llm_func: Callable,
    extract_type: str = "both"  # "hl", "ll", "both"
) -> Tuple[List[str], List[str]]:
    """
    ä»æŸ¥è¯¢ä¸­æå–åŒå±‚å…³é”®è¯
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        llm_func: LLMå‡½æ•°
        extract_type: æå–ç±»å‹ (hl=é«˜çº§, ll=ä½çº§, both=åŒå±‚)
    
    Returns:
        Tuple[List[str], List[str]]: (high_level_keywords, low_level_keywords)
    """
    
    hl_keywords, ll_keywords = [], []
    
    if extract_type in ["hl", "both"]:
        # é«˜çº§å…³é”®è¯æå– (æ¦‚å¿µæ€§ã€æŠ½è±¡æ€§)
        hl_prompt = f"""
        ä»ä»¥ä¸‹æŸ¥è¯¢ä¸­æå–é«˜çº§å…³é”®è¯ï¼Œå…³æ³¨æ¦‚å¿µæ€§ã€æŠ½è±¡æ€§çš„å†…å®¹ï¼š
        æŸ¥è¯¢: {query}
        
        è¯·æå–å‡ºå…³é”®çš„æ¦‚å¿µã€ä¸»é¢˜ã€æŠ½è±¡æ¦‚å¿µç­‰é«˜çº§å…³é”®è¯ã€‚
        è¿”å›æ ¼å¼: keyword1, keyword2, keyword3
        """
        hl_result = await llm_func(hl_prompt, keyword_extraction=True)
        hl_keywords = [kw.strip() for kw in hl_result.split(',') if kw.strip()]
    
    if extract_type in ["ll", "both"]:
        # ä½çº§å…³é”®è¯æå– (å…·ä½“å®ä½“ã€äº‹å®æ€§)
        ll_prompt = f"""
        ä»ä»¥ä¸‹æŸ¥è¯¢ä¸­æå–ä½çº§å…³é”®è¯ï¼Œå…³æ³¨å…·ä½“çš„å®ä½“ã€äººåã€åœ°åã€äº‹å®ç­‰ï¼š
        æŸ¥è¯¢: {query}
        
        è¯·æå–å‡ºå…·ä½“çš„å®ä½“åç§°ã€ä¸“æœ‰åè¯ã€äº‹å®æ€§å…³é”®è¯ç­‰ã€‚
        è¿”å›æ ¼å¼: keyword1, keyword2, keyword3
        """
        ll_result = await llm_func(ll_prompt, keyword_extraction=True)
        ll_keywords = [kw.strip() for kw in ll_result.split(',') if kw.strip()]
    
    return hl_keywords, ll_keywords
```

## ğŸ”§ å®ç°å¼€å‘è§„èŒƒ

### 1. SiliconFlowå®ç° (æ€§ä»·æ¯”ä¹‹é€‰)
```python
async def siliconflow_llm_func(
    prompt: str,
    system_prompt: str = None,
    history_messages: List[dict] = None,
    config: dict = None,
    **kwargs
) -> str:
    """SiliconFlow LLMå®ç°"""
    
    headers = {
        "Authorization": f"Bearer {config['api_key']}",
        "Content-Type": "application/json"
    }
    
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    
    # æ·»åŠ å†å²æ¶ˆæ¯
    if history_messages:
        messages.extend(history_messages)
    
    messages.append({"role": "user", "content": prompt})
    
    data = {
        "model": config['model'],
        "messages": messages,
        "temperature": config.get('temperature', 0.1),
        "max_tokens": config.get('max_tokens', 32000),
        "stream": False
    }
    
    # æ·»åŠ æµå¼å¤„ç†æ”¯æŒ
    if kwargs.get('stream', False):
        data['stream'] = True
        return await _siliconflow_stream_request(config['host'], data, headers)
    
    async with httpx.AsyncClient(timeout=240) as client:
        response = await client.post(
            f"{config['host']}/chat/completions",
            json=data,
            headers=headers
        )
        
        if response.status_code != 200:
            raise Exception(f"SiliconFlow APIé”™è¯¯: {response.status_code} - {response.text}")
        
        result = response.json()
        return result['choices'][0]['message']['content']
```

### 2. OpenRouterå®ç° (æ¨¡å‹èšåˆ)
```python
async def openrouter_llm_func(
    prompt: str,
    system_prompt: str = None,
    config: dict = None,
    **kwargs
) -> str:
    """OpenRouter LLMå®ç°"""
    
    headers = {
        "Authorization": f"Bearer {config['api_key']}",
        "HTTP-Referer": "https://lightrag.dev",  # OpenRouterè¦æ±‚
        "X-Title": "LightRAG",
        "Content-Type": "application/json"
    }
    
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    
    messages.append({"role": "user", "content": prompt})
    
    data = {
        "model": config['model'],
        "messages": messages,
        "temperature": config.get('temperature', 0.1),
        "max_tokens": config.get('max_tokens', 32000)
    }
    
    async with httpx.AsyncClient(timeout=240) as client:
        response = await client.post(
            f"{config['host']}/chat/completions",
            json=data,
            headers=headers
        )
        
        if response.status_code != 200:
            raise Exception(f"OpenRouter APIé”™è¯¯: {response.status_code} - {response.text}")
        
        result = response.json()
        return result['choices'][0]['message']['content']
```

### 3. åµŒå…¥æ¨¡å‹å®ç° (BAAI/bge-m3)
```python
async def siliconflow_embedding_func(texts: List[str]) -> np.ndarray:
    """SiliconFlowåµŒå…¥æ¨¡å‹å®ç°"""
    api_key = os.getenv('EMBEDDING_BINDING_API_KEY')
    model = os.getenv('EMBEDDING_MODEL', 'BAAI/bge-m3')
    host = os.getenv('EMBEDDING_BINDING_HOST', 'https://api.siliconflow.cn/v1')
    embedding_dim = int(os.getenv('EMBEDDING_DIM', 1024))
    
    if not api_key:
        raise ValueError("ç¼ºå°‘åµŒå…¥æ¨¡å‹APIå¯†é’¥")
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    # æ‰¹é‡å¤„ç†ï¼Œé¿å…å•æ¬¡è¯·æ±‚è¿‡å¤§
    batch_size = int(os.getenv('EMBEDDING_BATCH_NUM', 50))
    all_embeddings = []
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        
        data = {
            "model": model,
            "input": batch,
            "encoding_format": "float"
        }
        
        async with httpx.AsyncClient(timeout=240) as client:
            response = await client.post(
                f"{host}/embeddings",
                json=data,
                headers=headers
            )
            
            if response.status_code != 200:
                raise Exception(f"åµŒå…¥æ¨¡å‹APIé”™è¯¯: {response.status_code} - {response.text}")
            
            result = response.json()
            batch_embeddings = [item['embedding'] for item in result['data']]
            all_embeddings.extend(batch_embeddings)
    
    embeddings_array = np.array(all_embeddings)
    
    # éªŒè¯ç»´åº¦
    if embeddings_array.shape[1] != embedding_dim:
        logger.warning(f"åµŒå…¥ç»´åº¦ä¸åŒ¹é…: æœŸæœ›{embedding_dim}, å®é™…{embeddings_array.shape[1]}")
    
    return embeddings_array
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. æ™ºèƒ½ç¼“å­˜æœºåˆ¶
```python
import hashlib
from functools import lru_cache
from typing import Optional

class LLMCache:
    def __init__(self, redis_client=None):
        self.redis = redis_client
        self.local_cache = {}
        self.cache_enabled = os.getenv('ENABLE_LLM_CACHE', 'true').lower() == 'true'
        self.extract_cache_enabled = os.getenv('ENABLE_LLM_CACHE_FOR_EXTRACT', 'true').lower() == 'true'
    
    def get_cache_key(self, prompt: str, config: dict) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        cache_data = {
            'prompt': prompt,
            'model': config.get('model'),
            'temperature': config.get('temperature'),
            'max_tokens': config.get('max_tokens')
        }
        cache_str = json.dumps(cache_data, sort_keys=True)
        return f"llm_cache:{hashlib.md5(cache_str.encode()).hexdigest()}"
    
    async def get(self, cache_key: str) -> Optional[str]:
        """è·å–ç¼“å­˜"""
        if not self.cache_enabled:
            return None
        
        # ä¼˜å…ˆä»Redisè·å–
        if self.redis:
            try:
                cached = await self.redis.get(cache_key)
                if cached:
                    return cached
            except Exception as e:
                logger.warning(f"Redisç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        # é™çº§åˆ°æœ¬åœ°ç¼“å­˜
        return self.local_cache.get(cache_key)
    
    async def set(self, cache_key: str, value: str, expire: int = 86400) -> None:
        """è®¾ç½®ç¼“å­˜"""
        if not self.cache_enabled:
            return
        
        # å­˜å‚¨åˆ°Redis
        if self.redis:
            try:
                await self.redis.set(cache_key, value, ex=expire)
            except Exception as e:
                logger.warning(f"Redisç¼“å­˜å†™å…¥å¤±è´¥: {e}")
        
        # å­˜å‚¨åˆ°æœ¬åœ°ç¼“å­˜
        self.local_cache[cache_key] = value
        
        # é™åˆ¶æœ¬åœ°ç¼“å­˜å¤§å°
        if len(self.local_cache) > 1000:
            # åˆ é™¤æœ€è€çš„ç¼“å­˜é¡¹
            oldest_key = next(iter(self.local_cache))
            del self.local_cache[oldest_key]

# å…¨å±€ç¼“å­˜å®ä¾‹
llm_cache = LLMCache()

async def cached_llm_call(prompt: str, config: dict, **kwargs) -> str:
    """å¸¦ç¼“å­˜çš„LLMè°ƒç”¨"""
    cache_key = llm_cache.get_cache_key(prompt, config)
    
    # å°è¯•ä»ç¼“å­˜è·å–
    cached_result = await llm_cache.get(cache_key)
    if cached_result:
        logger.debug(f"LLMç¼“å­˜å‘½ä¸­: {cache_key[:16]}...")
        return cached_result
    
    # ç¼“å­˜æœªå‘½ä¸­ï¼Œè°ƒç”¨LLM
    result = await actual_llm_call(prompt, config, **kwargs)
    
    # ä¿å­˜åˆ°ç¼“å­˜
    await llm_cache.set(cache_key, result)
    
    return result
```

### 2. å¹¶å‘æ§åˆ¶
```python
import asyncio
from asyncio import Semaphore
from collections import defaultdict

# æ ¹æ®ç”¨æˆ·é…ç½®æ§åˆ¶å¹¶å‘
MAX_ASYNC = int(os.getenv('MAX_ASYNC', 16))
EMBEDDING_MAX_ASYNC = int(os.getenv('EMBEDDING_FUNC_MAX_ASYNC', 32))

# ä¸åŒä»»åŠ¡ç±»å‹çš„ä¿¡å·é‡
task_semaphores = {
    'entity_extraction': Semaphore(8),    # å®ä½“æå–ï¼šé«˜å¹¶å‘
    'keyword_extraction': Semaphore(8),   # å…³é”®è¯æå–ï¼šé«˜å¹¶å‘
    'entity_summary': Semaphore(4),       # å®ä½“æ‘˜è¦ï¼šä¸­ç­‰å¹¶å‘
    'relation_summary': Semaphore(4),     # å…³ç³»æ‘˜è¦ï¼šä¸­ç­‰å¹¶å‘
    'query_response': Semaphore(2),       # æŸ¥è¯¢å“åº”ï¼šä½å¹¶å‘
    'embedding': Semaphore(EMBEDDING_MAX_ASYNC)
}

async def controlled_llm_call(
    prompt: str, 
    task_type: str = 'default',
    **kwargs
) -> str:
    """å¸¦å¹¶å‘æ§åˆ¶çš„LLMè°ƒç”¨"""
    semaphore = task_semaphores.get(task_type, task_semaphores['query_response'])
    
    async with semaphore:
        return await cached_llm_call(prompt, **kwargs)
```

### 3. ä¼˜å…ˆçº§é˜Ÿåˆ—
```python
import heapq
from dataclasses import dataclass
from typing import Any, Callable

@dataclass
class LLMTask:
    priority: int  # æ•°å­—è¶Šå¤§ä¼˜å…ˆçº§è¶Šé«˜
    task_id: str
    prompt: str
    config: dict
    callback: Callable
    timestamp: float = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = time.time()
    
    def __lt__(self, other):
        # ä¼˜å…ˆçº§é«˜çš„æ’åœ¨å‰é¢ï¼Œå¦‚æœä¼˜å…ˆçº§ç›¸åŒåˆ™æŒ‰æ—¶é—´æ’åº
        if self.priority != other.priority:
            return self.priority > other.priority
        return self.timestamp < other.timestamp

class LLMTaskScheduler:
    def __init__(self):
        self.task_queue = []
        self.running_tasks = set()
        self.max_concurrent = MAX_ASYNC
        
    async def submit_task(self, task: LLMTask):
        """æäº¤ä»»åŠ¡åˆ°ä¼˜å…ˆçº§é˜Ÿåˆ—"""
        heapq.heappush(self.task_queue, task)
        await self._process_queue()
    
    async def _process_queue(self):
        """å¤„ç†ä»»åŠ¡é˜Ÿåˆ—"""
        while (len(self.running_tasks) < self.max_concurrent 
               and self.task_queue):
            
            task = heapq.heappop(self.task_queue)
            self.running_tasks.add(task.task_id)
            
            # å¼‚æ­¥æ‰§è¡Œä»»åŠ¡
            asyncio.create_task(self._execute_task(task))
    
    async def _execute_task(self, task: LLMTask):
        """æ‰§è¡ŒLLMä»»åŠ¡"""
        try:
            result = await cached_llm_call(task.prompt, task.config)
            await task.callback(result)
        except Exception as e:
            logger.error(f"LLMä»»åŠ¡æ‰§è¡Œå¤±è´¥ {task.task_id}: {e}")
        finally:
            self.running_tasks.discard(task.task_id)
            # ç»§ç»­å¤„ç†é˜Ÿåˆ—
            await self._process_queue()

# å…¨å±€ä»»åŠ¡è°ƒåº¦å™¨
llm_scheduler = LLMTaskScheduler()
```

## ğŸ” é”™è¯¯å¤„ç†å’Œé‡è¯•

### 1. æ™ºèƒ½é‡è¯•æœºåˆ¶
```python
import backoff
from typing import Type, Tuple

class LLMError(Exception):
    """LLMè°ƒç”¨é”™è¯¯åŸºç±»"""
    pass

class LLMRateLimitError(LLMError):
    """é€Ÿç‡é™åˆ¶é”™è¯¯"""
    pass

class LLMServerError(LLMError):
    """æœåŠ¡å™¨é”™è¯¯"""
    pass

@backoff.on_exception(
    backoff.expo,
    (httpx.RequestError, httpx.HTTPStatusError, LLMServerError),
    max_tries=3,
    max_time=300,
    giveup=lambda e: isinstance(e, LLMRateLimitError)
)
async def robust_llm_call(prompt: str, config: dict, **kwargs) -> str:
    """å¸¦é‡è¯•æœºåˆ¶çš„LLMè°ƒç”¨"""
    try:
        return await llm_func(prompt, config, **kwargs)
    except httpx.HTTPStatusError as e:
        if e.response.status_code == 429:  # Rate limit
            raise LLMRateLimitError(f"APIé€Ÿç‡é™åˆ¶: {e.response.text}")
        elif e.response.status_code >= 500:  # Server error
            raise LLMServerError(f"æœåŠ¡å™¨é”™è¯¯: {e.response.text}")
        else:
            logger.error(f"HTTPé”™è¯¯ {e.response.status_code}: {e.response.text}")
            raise LLMError(f"APIè°ƒç”¨å¤±è´¥: {e.response.text}")
    except Exception as e:
        logger.error(f"LLMè°ƒç”¨æ„å¤–é”™è¯¯: {e}")
        raise LLMError(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")
```

### 2. é™çº§ç­–ç•¥
```python
async def fallback_llm_call(
    prompt: str, 
    primary_config: dict,
    fallback_config: dict = None,
    **kwargs
) -> str:
    """LLMè°ƒç”¨é™çº§ç­–ç•¥"""
    
    # å°è¯•ä¸»è¦æ¨¡å‹
    try:
        return await robust_llm_call(prompt, primary_config, **kwargs)
    except LLMRateLimitError:
        # é€Ÿç‡é™åˆ¶æ—¶ç­‰å¾…é‡è¯•
        await asyncio.sleep(60)
        raise
    except Exception as e:
        logger.warning(f"ä¸»è¦LLMå¤±è´¥: {e}")
        
        # å¦‚æœæœ‰é™çº§é…ç½®ï¼Œå°è¯•é™çº§æ¨¡å‹
        if fallback_config:
            try:
                logger.info(f"åˆ‡æ¢åˆ°é™çº§æ¨¡å‹: {fallback_config['model']}")
                return await robust_llm_call(prompt, fallback_config, **kwargs)
            except Exception as fallback_error:
                logger.error(f"é™çº§æ¨¡å‹ä¹Ÿå¤±è´¥: {fallback_error}")
        
        # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºåŸå§‹é”™è¯¯
        raise e
```

## ğŸ”§ é…ç½®ç®¡ç†

### ç¯å¢ƒå˜é‡é…ç½®æ˜ å°„
```python
def load_llm_config() -> dict:
    """ä»ç¯å¢ƒå˜é‡åŠ è½½LLMé…ç½®"""
    config = {}
    
    # å…¨å±€é»˜è®¤é…ç½®
    config['default'] = {
        'binding': os.getenv('LLM_BINDING', 'siliconflow'),
        'model': os.getenv('LLM_MODEL', 'Qwen/Qwen2.5-14B-Instruct'),
        'host': os.getenv('LLM_BINDING_HOST', 'https://api.siliconflow.cn/v1'),
        'api_key': os.getenv('LLM_BINDING_API_KEY'),
        'temperature': float(os.getenv('TEMPERATURE', 0.1)),
        'timeout': int(os.getenv('TIMEOUT', 240)),
        'max_tokens': int(os.getenv('MAX_TOKENS', 32000))
    }
    
    # ä»»åŠ¡ç‰¹åŒ–é…ç½®
    for task in ['entity_extraction', 'entity_summary', 'relation_summary', 
                 'query_response', 'keyword_extraction']:
        task_config = {}
        prefix = task.upper()
        
        for key in ['BINDING', 'MODEL', 'BINDING_HOST', 'BINDING_API_KEY']:
            env_key = f"{prefix}_LLM_{key}"
            value = os.getenv(env_key)
            if value:
                config_key = key.lower().replace('binding_', '')
                task_config[config_key] = value
        
        if task_config:
            config[task] = {**config['default'], **task_config}
    
    # åµŒå…¥æ¨¡å‹é…ç½®
    config['embedding'] = {
        'binding': os.getenv('EMBEDDING_BINDING', 'siliconflow'),
        'model': os.getenv('EMBEDDING_MODEL', 'BAAI/bge-m3'),
        'host': os.getenv('EMBEDDING_BINDING_HOST', 'https://api.siliconflow.cn/v1'),
        'api_key': os.getenv('EMBEDDING_BINDING_API_KEY'),
        'dim': int(os.getenv('EMBEDDING_DIM', 1024)),
        'batch_size': int(os.getenv('EMBEDDING_BATCH_NUM', 50))
    }
    
    return config
```

## ğŸ§ª æµ‹è¯•è§„èŒƒ

### 1. LLMåŠŸèƒ½æµ‹è¯•
```python
@pytest.mark.asyncio
async def test_llm_functionality():
    """æµ‹è¯•LLMåŸºæœ¬åŠŸèƒ½"""
    config = {
        'binding': 'siliconflow',
        'model': 'Qwen/Qwen2.5-7B-Instruct',
        'api_key': 'test_key'
    }
    
    prompt = "æµ‹è¯•LLMåŠŸèƒ½"
    response = await siliconflow_llm_func(prompt, config=config)
    
    assert isinstance(response, str)
    assert len(response) > 0

@pytest.mark.asyncio
async def test_task_specialization():
    """æµ‹è¯•ä»»åŠ¡ç‰¹åŒ–é…ç½®"""
    tasks = ['entity_extraction', 'entity_summary', 'query_response']
    
    for task in tasks:
        llm_func = get_llm_func(task)
        assert llm_func is not None
        
        # æµ‹è¯•ä¸åŒä»»åŠ¡çš„å“åº”
        response = await llm_func("æµ‹è¯•ä»»åŠ¡ç‰¹åŒ–")
        assert isinstance(response, str)
```

### 2. åµŒå…¥åŠŸèƒ½æµ‹è¯•
```python
@pytest.mark.asyncio
async def test_embedding_functionality():
    """æµ‹è¯•åµŒå…¥æ¨¡å‹åŠŸèƒ½"""
    texts = ["æµ‹è¯•æ–‡æœ¬1", "æµ‹è¯•æ–‡æœ¬2", "æµ‹è¯•æ–‡æœ¬3"]
    embeddings = await siliconflow_embedding_func(texts)
    
    expected_dim = int(os.getenv('EMBEDDING_DIM', 1024))
    
    assert embeddings.shape[0] == len(texts)
    assert embeddings.shape[1] == expected_dim
    assert np.all(np.isfinite(embeddings))  # ç¡®ä¿æ²¡æœ‰NaNæˆ–Infå€¼
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **APIå¯†é’¥å®‰å…¨**: ä¸è¦åœ¨ä»£ç ä¸­ç¡¬ç¼–ç APIå¯†é’¥ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡
2. **ä»»åŠ¡ç‰¹åŒ–**: åˆç†é…ç½®ä¸åŒä»»åŠ¡çš„æ¨¡å‹ä»¥ä¼˜åŒ–æˆæœ¬å’Œæ€§èƒ½
3. **é€Ÿç‡é™åˆ¶**: éµå®ˆå„æä¾›å•†çš„APIè°ƒç”¨é™åˆ¶ï¼Œå®ç°æ™ºèƒ½é‡è¯•
4. **æˆæœ¬æ§åˆ¶**: ç›‘æ§APIè°ƒç”¨æˆæœ¬ï¼Œåˆç†é€‰æ‹©æ¨¡å‹
5. **é”™è¯¯å¤„ç†**: å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œé™çº§æœºåˆ¶
6. **æ¨¡å‹ä¸€è‡´æ€§**: åµŒå…¥æ¨¡å‹åœ¨æ•´ä¸ªç³»ç»Ÿä¸­ä¿æŒä¸€è‡´
7. **ç¼“å­˜ç­–ç•¥**: åˆç†ä½¿ç”¨ç¼“å­˜å‡å°‘é‡å¤è°ƒç”¨
8. **å¹¶å‘æ§åˆ¶**: æ ¹æ®ä»»åŠ¡ç±»å‹è®¾ç½®åˆé€‚çš„å¹¶å‘é™åˆ¶
9. **ä¼˜å…ˆçº§ç®¡ç†**: é‡è¦ä»»åŠ¡è®¾ç½®æ›´é«˜ä¼˜å…ˆçº§
10. **è¶…æ—¶è®¾ç½®**: åˆç†è®¾ç½®è¶…æ—¶æ—¶é—´é¿å…é•¿æ—¶é—´ç­‰å¾…
