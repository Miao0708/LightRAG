---
alwaysApply: true
---

# LightRAG 项目开发规则

## 📋 项目概览

LightRAG 是由香港大学数据科学实验室开发的轻量级检索增强生成(RAG)系统，巧妙结合知识图谱和向量检索技术，在保持高效性能的同时，显著提升复杂问答的准确性。

### 核心特点
- 🔍 **六种查询模式**: Naive/Local/Global/Hybrid/Mix/Bypass 灵活检索策略
- 📊 **知识图谱增强**: 实体关系抽取 + 图结构索引 + 双层检索机制
- ⚡ **轻量级设计**: 相比GraphRAG更快更经济，成本效率高
- 🔄 **增量更新**: 支持实时数据更新，无需重建整个索引
- 🗄️ **多存储后端**: 支持从本地存储到云端数据库的无缝切换
- 🎯 **任务特化配置**: 不同AI任务使用不同模型，成本与性能最佳平衡

## 📁 项目结构

```
LightRAG/
├── lightrag/                    # 核心库
│   ├── lightrag.py             # 主入口类
│   ├── llm/                    # LLM适配器 (支持10+提供商)
│   ├── kg/                     # 存储后端实现
│   ├── api/                    # FastAPI服务 + Ollama兼容
│   ├── rerank.py               # 重排序集成
│   └── tools/                  # 工具包 (可视化等)
├── lightrag_webui/             # React前端界面
├── examples/                   # 示例代码
├── docs/                       # 完整项目文档
├── k8s-deploy/                 # Kubernetes部署
└── reproduce/                  # 性能基准测试
```

## 🛠️ 开发环境配置

参考用户的本地配置和最新文档:
- **数据存储**: Redis(KV) + PostgreSQL(Doc/Vector) + Neo4j(Graph)
- **LLM任务特化**: 
  - 实体提取: SiliconFlow Qwen2.5-7B (快速+经济)
  - 实体总结: SiliconFlow Qwen2.5-72B (理解能力强)
  - 查询响应: OpenRouter Qwen3-235B (对话能力强)
- **嵌入模型**: BAAI/bge-m3 (1024维)
- **重排序**: BAAI/bge-reranker-v2-m3 (可选，提高检索质量)

## 🔧 开发规范

### 代码风格
- 使用中文注释和文档
- 遵循Python PEP8规范
- 异步操作优先使用async/await
- 错误处理要完善，包含重试机制

### 配置管理
- 环境变量统一使用 `.env` 文件
- 支持任务特化的多模型配置
- 敏感信息不要硬编码
- 支持多环境配置(开发/测试/生产)

### 性能优化
- 优先使用缓存机制(ENABLE_LLM_CACHE=true)
- 合理设置并发参数(MAX_ASYNC=16)
- 启用重排序优化检索质量
- 根据查询类型选择最优检索模式

## 🔍 六种查询模式

### 模式对比表
| 模式 | 检索方式 | 响应时间 | 准确率 | 资源消耗 | 适用场景 |
|------|----------|----------|--------|----------|----------|
| **Naive** | 纯向量 | 0.8s | 72% | 低 | 简单事实查询 |
| **Local** | 实体邻域 | 1.2s | 78% | 中 | 特定主题查询 |
| **Global** | 关系网络 | 2.1s | 82% | 中高 | 概念性查询 |
| **Hybrid** | 实体+关系 | 1.8s | 85% | 中高 | 平衡性查询 |
| **Mix** | 图谱+向量 | 2.5s | 88% | 高 | 复杂推理查询 |
| **Bypass** | 直接LLM | 最快 | 依赖LLM | 最低 | 通用对话 |

### 智能模式选择
```python
# 查询模式自动选择示例
def auto_select_mode(query: str) -> str:
    if any(word in query for word in ["你好", "谢谢", "再见"]):
        return "bypass"  # 普通对话
    elif any(word in query for word in ["什么是", "定义", "概念"]):
        return "global"  # 概念查询
    elif any(word in query for word in ["详细介绍", "具体说明"]):
        return "local"   # 实体详情
    elif any(word in query for word in ["比较", "对比", "分析"]):
        return "mix"     # 复杂推理
    else:
        return "hybrid"  # 平衡选择
```

## 🚀 常用开发任务

### 添加新的存储后端
1. 在 `lightrag/kg/` 创建新的实现文件
2. 继承对应的抽象基类
3. 实现所有必需的异步方法
4. 在配置中添加对应的环境变量

### 添加新的LLM提供商
1. 在 `lightrag/llm/` 创建新的适配器
2. 实现统一的LLM接口
3. 支持流式和批量处理
4. 添加错误处理和重试逻辑

### 集成重排序模型
```python
from lightrag.rerank import custom_rerank

async def my_rerank_func(query: str, documents: list, top_n: int = None, **kwargs):
    return await custom_rerank(
        query=query,
        documents=documents,
        model="BAAI/bge-reranker-v2-m3",
        base_url="https://api.siliconflow.cn/v1",
        api_key="your_api_key",
        top_n=top_n or 10
    )
```

### 本地开发启动
```bash
# 启动API服务器
python -m lightrag.api.lightrag_server

# 启动Web UI (需要单独启动)
cd lightrag_webui && npm run dev

# Docker快速启动
docker-compose up -d
```

## ⚠️ 重要注意事项

1. **嵌入模型一致性**: 一旦选定嵌入模型，查询时必须使用相同模型
2. **查询模式选择**: 根据查询类型和性能要求选择合适的模式
3. **任务特化配置**: 不同AI任务配置不同模型以优化成本
4. **数据库连接**: 确保所有数据库服务正常运行
5. **API密钥安全**: 不要在代码中硬编码API密钥
6. **内存管理**: 大文档处理时注意内存使用
7. **并发控制**: 避免过度并发导致API限流
8. **重排序开关**: 可通过enable_rerank参数控制是否启用重排序

## 🔗 相关文档
- [项目概述](docs/01-project-overview.md)
- [架构指南](docs/02-architecture-guide.md)
- [LLM配置](docs/05-llm-configuration.md)
- [查询模式分析](docs/09-query-modes-analysis.md)
- [重排序集成](docs/12-rerank-integration.md)
- [Docker部署](docs/13-docker-deployment.md)
# LightRAG 项目开发规则

## 📋 项目概览

LightRAG 是由香港大学数据科学实验室开发的轻量级检索增强生成(RAG)系统，巧妙结合知识图谱和向量检索技术，在保持高效性能的同时，显著提升复杂问答的准确性。

### 核心特点
- 🔍 **六种查询模式**: Naive/Local/Global/Hybrid/Mix/Bypass 灵活检索策略
- 📊 **知识图谱增强**: 实体关系抽取 + 图结构索引 + 双层检索机制
- ⚡ **轻量级设计**: 相比GraphRAG更快更经济，成本效率高
- 🔄 **增量更新**: 支持实时数据更新，无需重建整个索引
- 🗄️ **多存储后端**: 支持从本地存储到云端数据库的无缝切换
- 🎯 **任务特化配置**: 不同AI任务使用不同模型，成本与性能最佳平衡

## 📁 项目结构

```
LightRAG/
├── lightrag/                    # 核心库
│   ├── lightrag.py             # 主入口类
│   ├── llm/                    # LLM适配器 (支持10+提供商)
│   ├── kg/                     # 存储后端实现
│   ├── api/                    # FastAPI服务 + Ollama兼容
│   ├── rerank.py               # 重排序集成
│   └── tools/                  # 工具包 (可视化等)
├── lightrag_webui/             # React前端界面
├── examples/                   # 示例代码
├── docs/                       # 完整项目文档
├── k8s-deploy/                 # Kubernetes部署
└── reproduce/                  # 性能基准测试
```

## 🛠️ 开发环境配置

参考用户的本地配置和最新文档:
- **数据存储**: Redis(KV) + PostgreSQL(Doc/Vector) + Neo4j(Graph)
- **LLM任务特化**: 
  - 实体提取: SiliconFlow Qwen2.5-7B (快速+经济)
  - 实体总结: SiliconFlow Qwen2.5-72B (理解能力强)
  - 查询响应: OpenRouter Qwen3-235B (对话能力强)
- **嵌入模型**: BAAI/bge-m3 (1024维)
- **重排序**: BAAI/bge-reranker-v2-m3 (可选，提高检索质量)

## 🔧 开发规范

### 代码风格
- 使用中文注释和文档
- 遵循Python PEP8规范
- 异步操作优先使用async/await
- 错误处理要完善，包含重试机制

### 配置管理
- 环境变量统一使用 `.env` 文件
- 支持任务特化的多模型配置
- 敏感信息不要硬编码
- 支持多环境配置(开发/测试/生产)

### 性能优化
- 优先使用缓存机制(ENABLE_LLM_CACHE=true)
- 合理设置并发参数(MAX_ASYNC=16)
- 启用重排序优化检索质量
- 根据查询类型选择最优检索模式

## 🔍 六种查询模式

### 模式对比表
| 模式 | 检索方式 | 响应时间 | 准确率 | 资源消耗 | 适用场景 |
|------|----------|----------|--------|----------|----------|
| **Naive** | 纯向量 | 0.8s | 72% | 低 | 简单事实查询 |
| **Local** | 实体邻域 | 1.2s | 78% | 中 | 特定主题查询 |
| **Global** | 关系网络 | 2.1s | 82% | 中高 | 概念性查询 |
| **Hybrid** | 实体+关系 | 1.8s | 85% | 中高 | 平衡性查询 |
| **Mix** | 图谱+向量 | 2.5s | 88% | 高 | 复杂推理查询 |
| **Bypass** | 直接LLM | 最快 | 依赖LLM | 最低 | 通用对话 |

### 智能模式选择
```python
# 查询模式自动选择示例
def auto_select_mode(query: str) -> str:
    if any(word in query for word in ["你好", "谢谢", "再见"]):
        return "bypass"  # 普通对话
    elif any(word in query for word in ["什么是", "定义", "概念"]):
        return "global"  # 概念查询
    elif any(word in query for word in ["详细介绍", "具体说明"]):
        return "local"   # 实体详情
    elif any(word in query for word in ["比较", "对比", "分析"]):
        return "mix"     # 复杂推理
    else:
        return "hybrid"  # 平衡选择
```

## 🚀 常用开发任务

### 添加新的存储后端
1. 在 `lightrag/kg/` 创建新的实现文件
2. 继承对应的抽象基类
3. 实现所有必需的异步方法
4. 在配置中添加对应的环境变量

### 添加新的LLM提供商
1. 在 `lightrag/llm/` 创建新的适配器
2. 实现统一的LLM接口
3. 支持流式和批量处理
4. 添加错误处理和重试逻辑

### 集成重排序模型
```python
from lightrag.rerank import custom_rerank

async def my_rerank_func(query: str, documents: list, top_n: int = None, **kwargs):
    return await custom_rerank(
        query=query,
        documents=documents,
        model="BAAI/bge-reranker-v2-m3",
        base_url="https://api.siliconflow.cn/v1",
        api_key="your_api_key",
        top_n=top_n or 10
    )
```

### 本地开发启动
```bash
# 启动API服务器
python -m lightrag.api.lightrag_server

# 启动Web UI (需要单独启动)
cd lightrag_webui && npm run dev

# Docker快速启动
docker-compose up -d
```

## ⚠️ 重要注意事项

1. **嵌入模型一致性**: 一旦选定嵌入模型，查询时必须使用相同模型
2. **查询模式选择**: 根据查询类型和性能要求选择合适的模式
3. **任务特化配置**: 不同AI任务配置不同模型以优化成本
4. **数据库连接**: 确保所有数据库服务正常运行
5. **API密钥安全**: 不要在代码中硬编码API密钥
6. **内存管理**: 大文档处理时注意内存使用
7. **并发控制**: 避免过度并发导致API限流
8. **重排序开关**: 可通过enable_rerank参数控制是否启用重排序

## 🔗 相关文档
- [项目概述](docs/01-project-overview.md)
- [架构指南](docs/02-architecture-guide.md)
- [LLM配置](docs/05-llm-configuration.md)
- [查询模式分析](docs/09-query-modes-analysis.md)
- [重排序集成](docs/12-rerank-integration.md)
- [Docker部署](docs/13-docker-deployment.md)
